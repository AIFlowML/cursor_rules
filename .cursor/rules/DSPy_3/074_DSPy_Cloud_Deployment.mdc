# DSPy Cloud Deployment Patterns

Comprehensive cloud-native deployment patterns for DSPy 3.0.1 applications across AWS, GCP, and Azure platforms.

## Quick Start

**Instant AWS Lambda Deployment**
```python
# aws_lambda_handler.py
import json
import dspy
from dspy.utils import usage_tracker

def lambda_handler(event, context):
    # Initialize DSPy with OpenAI (serverless-friendly)
    lm = dspy.LM('openai/gpt-4o-mini', api_key=os.environ['OPENAI_API_KEY'])
    dspy.configure(lm=lm)
    
    # Create optimized signature for cold starts
    class ProcessQuery(dspy.Signature):
        """Process user query efficiently"""
        query: str = dspy.InputField()
        response: str = dspy.OutputField()
    
    # Use CoT for single-shot processing
    processor = dspy.ChainOfThought(ProcessQuery)
    
    try:
        with usage_tracker.track() as tracker:
            result = processor(query=event['query'])
            
        return {
            'statusCode': 200,
            'headers': {'Content-Type': 'application/json'},
            'body': json.dumps({
                'response': result.response,
                'cost': tracker.cost,
                'tokens': tracker.total_tokens
            })
        }
    except Exception as e:
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }

# requirements.txt
dspy-ai>=2.5.0
openai>=1.0.0
```

**Instant Google Cloud Run Deployment**
```python
# main.py
import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import dspy
from dspy.utils import usage_tracker

app = FastAPI(title="DSPy Cloud Run Service")

# Initialize DSPy with Vertex AI
@app.on_event("startup")
async def startup_event():
    lm = dspy.LM(
        'vertex_ai/gemini-1.5-pro',
        project=os.environ['GCP_PROJECT'],
        region=os.environ['GCP_REGION']
    )
    dspy.configure(lm=lm)

class QueryRequest(BaseModel):
    query: str
    context: str = ""

@app.post("/process")
async def process_query(request: QueryRequest):
    class QueryProcessor(dspy.Signature):
        """Process query with context"""
        query: str = dspy.InputField()
        context: str = dspy.InputField()
        response: str = dspy.OutputField()
    
    processor = dspy.ChainOfThought(QueryProcessor)
    
    try:
        with usage_tracker.track() as tracker:
            result = processor(
                query=request.query,
                context=request.context
            )
        
        return {
            "response": result.response,
            "usage": {
                "cost": tracker.cost,
                "tokens": tracker.total_tokens
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8080
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"]
```

**Instant Azure Container Apps Deployment**
```python
# azure_app.py
import os
import asyncio
from fastapi import FastAPI, BackgroundTasks
from azure.identity import DefaultAzureCredential
from azure.keyvault.secrets import SecretClient
import dspy

app = FastAPI()

# Azure Key Vault integration
credential = DefaultAzureCredential()
vault_url = f"https://{os.environ['KEY_VAULT_NAME']}.vault.azure.net/"
secret_client = SecretClient(vault_url=vault_url, credential=credential)

@app.on_event("startup")
async def startup():
    # Retrieve secrets from Azure Key Vault
    api_key = secret_client.get_secret("openai-api-key").value
    
    lm = dspy.LM('openai/gpt-4o', api_key=api_key)
    dspy.configure(lm=lm)

@app.post("/analyze")
async def analyze_data(data: dict, background_tasks: BackgroundTasks):
    class DataAnalyzer(dspy.Signature):
        """Analyze data and provide insights"""
        data: str = dspy.InputField()
        analysis: str = dspy.OutputField()
        recommendations: str = dspy.OutputField()
    
    analyzer = dspy.ChainOfThought(DataAnalyzer)
    
    # Process asynchronously
    async def process():
        result = analyzer(data=str(data))
        # Log to Azure Application Insights
        # telemetry_client.track_event('analysis_completed')
        return result
    
    result = await process()
    return {
        "analysis": result.analysis,
        "recommendations": result.recommendations
    }

# bicep/container-app.bicep
param containerAppName string
param location string = resourceGroup().location

resource containerApp 'Microsoft.App/containerApps@2023-05-01' = {
  name: containerAppName
  location: location
  properties: {
    configuration: {
      ingress: {
        external: true
        targetPort: 8080
      }
    }
    template: {
      containers: [{
        name: 'dspy-app'
        image: 'your-registry.azurecr.io/dspy-app:latest'
        resources: {
          cpu: json('0.5')
          memory: '1Gi'
        }
      }]
    }
  }
}
```

## Comprehensive Patterns

### AWS Multi-Service Architecture

**ECS Fargate with ALB**
```python
# ecs_service.py
import os
import boto3
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
import dspy
from dspy.utils import usage_tracker
import asyncio
import aioredis

app = FastAPI(title="DSPy ECS Service")

# Add CORS middleware for ALB health checks
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# AWS services
ssm = boto3.client('ssm')
cloudwatch = boto3.client('cloudwatch')
redis_client = None

@app.on_event("startup")
async def startup_event():
    global redis_client
    
    # Get configuration from SSM Parameter Store
    openai_key = ssm.get_parameter(
        Name='/dspy/openai/api_key',
        WithDecryption=True
    )['Parameter']['Value']
    
    redis_url = ssm.get_parameter(
        Name='/dspy/redis/url'
    )['Parameter']['Value']
    
    # Initialize Redis for caching
    redis_client = await aioredis.from_url(redis_url)
    
    # Configure DSPy
    lm = dspy.LM('openai/gpt-4o', api_key=openai_key)
    dspy.configure(lm=lm)

@app.get("/health")
async def health_check():
    """ALB health check endpoint"""
    return {"status": "healthy", "service": "dspy-ecs"}

@app.post("/process")
async def process_request(request: dict):
    request_id = request.get('request_id', 'unknown')
    
    # Check cache first
    cache_key = f"dspy:result:{hash(str(request))}"
    cached_result = await redis_client.get(cache_key)
    
    if cached_result:
        return json.loads(cached_result)
    
    class RequestProcessor(dspy.Signature):
        """Process business request efficiently"""
        request_data: str = dspy.InputField()
        processed_result: str = dspy.OutputField()
        confidence: float = dspy.OutputField()
    
    processor = dspy.ChainOfThought(RequestProcessor)
    
    try:
        with usage_tracker.track() as tracker:
            result = processor(request_data=str(request))
        
        response = {
            "result": result.processed_result,
            "confidence": result.confidence,
            "cost": tracker.cost,
            "request_id": request_id
        }
        
        # Cache result for 1 hour
        await redis_client.setex(
            cache_key, 
            3600, 
            json.dumps(response)
        )
        
        # Send metrics to CloudWatch
        cloudwatch.put_metric_data(
            Namespace='DSPy/ECS',
            MetricData=[
                {
                    'MetricName': 'RequestProcessed',
                    'Value': 1,
                    'Unit': 'Count'
                },
                {
                    'MetricName': 'ProcessingCost',
                    'Value': tracker.cost,
                    'Unit': 'None'
                }
            ]
        )
        
        return response
        
    except Exception as e:
        # Log error to CloudWatch
        cloudwatch.put_metric_data(
            Namespace='DSPy/ECS',
            MetricData=[{
                'MetricName': 'ProcessingError',
                'Value': 1,
                'Unit': 'Count'
            }]
        )
        raise HTTPException(status_code=500, detail=str(e))

# terraform/ecs.tf
resource "aws_ecs_cluster" "dspy_cluster" {
  name = "dspy-production"
  
  setting {
    name  = "containerInsights"
    value = "enabled"
  }
}

resource "aws_ecs_service" "dspy_service" {
  name            = "dspy-service"
  cluster         = aws_ecs_cluster.dspy_cluster.id
  task_definition = aws_ecs_task_definition.dspy_task.arn
  desired_count   = 3
  launch_type     = "FARGATE"
  
  network_configuration {
    subnets         = var.private_subnet_ids
    security_groups = [aws_security_group.dspy_service.id]
  }
  
  load_balancer {
    target_group_arn = aws_lb_target_group.dspy_tg.arn
    container_name   = "dspy-app"
    container_port   = 8080
  }
  
  depends_on = [aws_lb_listener.dspy_listener]
}

resource "aws_ecs_task_definition" "dspy_task" {
  family                   = "dspy-task"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = "1024"
  memory                   = "2048"
  execution_role_arn       = aws_iam_role.ecs_execution_role.arn
  task_role_arn           = aws_iam_role.ecs_task_role.arn
  
  container_definitions = jsonencode([{
    name  = "dspy-app"
    image = "${var.ecr_repository_url}:latest"
    
    portMappings = [{
      containerPort = 8080
      protocol      = "tcp"
    }]
    
    environment = [
      {
        name  = "AWS_DEFAULT_REGION"
        value = var.aws_region
      }
    ]
    
    logConfiguration = {
      logDriver = "awslogs"
      options = {
        awslogs-group         = aws_cloudwatch_log_group.dspy_logs.name
        awslogs-region        = var.aws_region
        awslogs-stream-prefix = "ecs"
      }
    }
    
    healthCheck = {
      command     = ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval    = 30
      timeout     = 5
      retries     = 3
      startPeriod = 60
    }
  }])
}
```

### Google Cloud Platform Architecture

**GKE with Workload Identity**
```python
# gke_deployment.py
import os
from google.cloud import secretmanager
from google.cloud import monitoring_v3
from kubernetes import client, config
import dspy
from dspy.utils import usage_tracker

# Kubernetes deployment
class DSPyGKEService:
    def __init__(self):
        self.secrets_client = secretmanager.SecretManagerServiceClient()
        self.monitoring_client = monitoring_v3.MetricServiceClient()
        self.project_id = os.environ['GOOGLE_CLOUD_PROJECT']
        
    def get_secret(self, secret_name: str) -> str:
        """Retrieve secret from Secret Manager"""
        name = f"projects/{self.project_id}/secrets/{secret_name}/versions/latest"
        response = self.secrets_client.access_secret_version(request={"name": name})
        return response.payload.data.decode("UTF-8")
    
    async def setup_dspy(self):
        """Initialize DSPy with GCP credentials"""
        openai_key = self.get_secret("openai-api-key")
        
        lm = dspy.LM('openai/gpt-4o', api_key=openai_key)
        dspy.configure(lm=lm)
    
    async def send_metrics(self, metric_name: str, value: float):
        """Send custom metrics to Cloud Monitoring"""
        series = monitoring_v3.TimeSeries()
        series.metric.type = f"custom.googleapis.com/dspy/{metric_name}"
        series.resource.type = "k8s_container"
        
        point = series.points.add()
        point.value.double_value = value
        point.interval.end_time.seconds = int(time.time())
        
        self.monitoring_client.create_time_series(
            name=f"projects/{self.project_id}",
            time_series=[series]
        )

# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dspy-service
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: dspy-service
  template:
    metadata:
      labels:
        app: dspy-service
    spec:
      serviceAccountName: dspy-workload-identity
      containers:
      - name: dspy-app
        image: gcr.io/PROJECT_ID/dspy-app:latest
        ports:
        - containerPort: 8080
        env:
        - name: GOOGLE_CLOUD_PROJECT
          value: "your-project-id"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: dspy-service
spec:
  selector:
    app: dspy-service
  ports:
  - port: 80
    targetPort: 8080
  type: LoadBalancer
---
apiVersion: networking.gke.io/v1
kind: ManagedCertificate
metadata:
  name: dspy-ssl-cert
spec:
  domains:
  - api.yourdomain.com
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: dspy-ingress
  annotations:
    kubernetes.io/ingress.global-static-ip-name: dspy-ip
    networking.gke.io/managed-certificates: dspy-ssl-cert
    kubernetes.io/ingress.class: "gce"
spec:
  rules:
  - host: api.yourdomain.com
    http:
      paths:
      - path: /*
        pathType: ImplementationSpecific
        backend:
          service:
            name: dspy-service
            port:
              number: 80
```

### Azure Kubernetes Service (AKS) with Azure AD

**AKS Production Deployment**
```python
# aks_service.py
import os
from azure.identity import DefaultAzureCredential
from azure.keyvault.secrets import SecretClient
from azure.monitor.opentelemetry import configure_azure_monitor
from azure.mgmt.monitor import MonitorManagementClient
import dspy
from dspy.utils import usage_tracker

class DSPyAKSService:
    def __init__(self):
        self.credential = DefaultAzureCredential()
        self.vault_url = f"https://{os.environ['KEY_VAULT_NAME']}.vault.azure.net/"
        self.secret_client = SecretClient(
            vault_url=self.vault_url, 
            credential=self.credential
        )
        
        # Configure Azure Monitor for telemetry
        configure_azure_monitor(
            connection_string=os.environ['APPLICATIONINSIGHTS_CONNECTION_STRING']
        )
    
    async def initialize_dspy(self):
        """Initialize DSPy with Azure Key Vault secrets"""
        try:
            openai_key = self.secret_client.get_secret("openai-api-key").value
            
            lm = dspy.LM('openai/gpt-4o', api_key=openai_key)
            dspy.configure(lm=lm)
            
            return True
        except Exception as e:
            logging.error(f"Failed to initialize DSPy: {e}")
            return False
    
    async def process_with_monitoring(self, request_data: dict):
        """Process request with Azure monitoring"""
        with usage_tracker.track() as tracker:
            class AzureRequestProcessor(dspy.Signature):
                """Process requests with Azure integration"""
                request: str = dspy.InputField()
                region: str = dspy.InputField()
                processed_response: str = dspy.OutputField()
                confidence_score: float = dspy.OutputField()
            
            processor = dspy.ChainOfThought(AzureRequestProcessor)
            
            result = processor(
                request=str(request_data),
                region=os.environ.get('AZURE_REGION', 'eastus')
            )
            
            # Custom telemetry
            from azure.monitor.opentelemetry import tracer
            with tracer.start_as_current_span("dspy_processing") as span:
                span.set_attribute("cost", tracker.cost)
                span.set_attribute("tokens", tracker.total_tokens)
                span.set_attribute("confidence", result.confidence_score)
            
            return {
                "response": result.processed_response,
                "confidence": result.confidence_score,
                "usage": {
                    "cost": tracker.cost,
                    "tokens": tracker.total_tokens
                }
            }

# bicep/aks-cluster.bicep
param clusterName string
param location string = resourceGroup().location
param nodeCount int = 3

resource aks 'Microsoft.ContainerService/managedClusters@2023-10-01' = {
  name: clusterName
  location: location
  identity: {
    type: 'SystemAssigned'
  }
  properties: {
    dnsPrefix: '${clusterName}-dns'
    agentPoolProfiles: [{
      name: 'agentpool'
      count: nodeCount
      vmSize: 'Standard_D4s_v3'
      osType: 'Linux'
      mode: 'System'
    }]
    servicePrincipalProfile: {
      clientId: 'msi'
    }
    addonProfiles: {
      azureKeyvaultSecretsProvider: {
        enabled: true
      }
      omsagent: {
        enabled: true
        config: {
          logAnalyticsWorkspaceResourceID: logAnalyticsWorkspace.id
        }
      }
    }
    networkProfile: {
      networkPlugin: 'azure'
      serviceCidr: '10.0.0.0/16'
      dnsServiceIP: '10.0.0.10'
    }
  }
}

# helm/dspy-app/values.yaml
replicaCount: 3

image:
  repository: youracr.azurecr.io/dspy-app
  pullPolicy: IfNotPresent
  tag: "latest"

service:
  type: LoadBalancer
  port: 80
  targetPort: 8080

ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/rate-limit: "100"
  hosts:
    - host: dspy-api.yourdomain.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: dspy-tls
      hosts:
        - dspy-api.yourdomain.com

resources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 1Gi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

nodeSelector: {}
tolerations: []
affinity: {}

secrets:
  keyVault:
    name: "dspy-keyvault"
    secrets:
      - secretName: "openai-api-key"
        key: "OPENAI_API_KEY"
```

## Core Implementation Patterns

### Multi-Cloud Deployment Strategy

**Cloud-Agnostic DSPy Service**
```python
# multi_cloud_service.py
import os
from abc import ABC, abstractmethod
import dspy
from dspy.utils import usage_tracker

class CloudProvider(ABC):
    @abstractmethod
    async def get_secret(self, name: str) -> str:
        pass
    
    @abstractmethod
    async def send_metrics(self, metric_name: str, value: float) -> None:
        pass
    
    @abstractmethod
    async def log_event(self, event: dict) -> None:
        pass

class AWSProvider(CloudProvider):
    def __init__(self):
        import boto3
        self.ssm = boto3.client('ssm')
        self.cloudwatch = boto3.client('cloudwatch')
    
    async def get_secret(self, name: str) -> str:
        response = self.ssm.get_parameter(
            Name=f'/dspy/{name}',
            WithDecryption=True
        )
        return response['Parameter']['Value']
    
    async def send_metrics(self, metric_name: str, value: float) -> None:
        self.cloudwatch.put_metric_data(
            Namespace='DSPy/MultiCloud',
            MetricData=[{
                'MetricName': metric_name,
                'Value': value,
                'Unit': 'None'
            }]
        )
    
    async def log_event(self, event: dict) -> None:
        # CloudWatch Logs implementation
        pass

class GCPProvider(CloudProvider):
    def __init__(self):
        from google.cloud import secretmanager
        from google.cloud import monitoring_v3
        self.secrets_client = secretmanager.SecretManagerServiceClient()
        self.monitoring_client = monitoring_v3.MetricServiceClient()
        self.project_id = os.environ['GOOGLE_CLOUD_PROJECT']
    
    async def get_secret(self, name: str) -> str:
        secret_name = f"projects/{self.project_id}/secrets/{name}/versions/latest"
        response = self.secrets_client.access_secret_version(
            request={"name": secret_name}
        )
        return response.payload.data.decode("UTF-8")
    
    async def send_metrics(self, metric_name: str, value: float) -> None:
        # Cloud Monitoring implementation
        pass
    
    async def log_event(self, event: dict) -> None:
        # Cloud Logging implementation
        pass

class AzureProvider(CloudProvider):
    def __init__(self):
        from azure.identity import DefaultAzureCredential
        from azure.keyvault.secrets import SecretClient
        self.credential = DefaultAzureCredential()
        vault_url = f"https://{os.environ['KEY_VAULT_NAME']}.vault.azure.net/"
        self.secret_client = SecretClient(
            vault_url=vault_url,
            credential=self.credential
        )
    
    async def get_secret(self, name: str) -> str:
        secret = self.secret_client.get_secret(name)
        return secret.value
    
    async def send_metrics(self, metric_name: str, value: float) -> None:
        # Azure Monitor implementation
        pass
    
    async def log_event(self, event: dict) -> None:
        # Application Insights implementation
        pass

class MultiCloudDSPyService:
    def __init__(self):
        # Auto-detect cloud provider
        if os.environ.get('AWS_REGION'):
            self.provider = AWSProvider()
            self.cloud = 'aws'
        elif os.environ.get('GOOGLE_CLOUD_PROJECT'):
            self.provider = GCPProvider()
            self.cloud = 'gcp'
        elif os.environ.get('AZURE_SUBSCRIPTION_ID'):
            self.provider = AzureProvider()
            self.cloud = 'azure'
        else:
            raise ValueError("Unknown cloud provider")
    
    async def initialize(self):
        """Initialize DSPy with cloud-specific configuration"""
        api_key = await self.provider.get_secret('openai-api-key')
        
        # Configure based on cloud capabilities
        if self.cloud == 'aws':
            # Use Bedrock if available
            try:
                lm = dspy.LM('bedrock/anthropic.claude-3-sonnet-20240229-v1:0')
            except:
                lm = dspy.LM('openai/gpt-4o', api_key=api_key)
        elif self.cloud == 'gcp':
            # Use Vertex AI if available
            try:
                lm = dspy.LM('vertex_ai/gemini-1.5-pro')
            except:
                lm = dspy.LM('openai/gpt-4o', api_key=api_key)
        else:
            # Azure OpenAI or fallback
            lm = dspy.LM('openai/gpt-4o', api_key=api_key)
        
        dspy.configure(lm=lm)
    
    async def process_request(self, request_data: dict):
        """Process request with cloud-native patterns"""
        class CloudOptimizedProcessor(dspy.Signature):
            """Process requests optimized for cloud deployment"""
            request: str = dspy.InputField()
            cloud_provider: str = dspy.InputField()
            result: str = dspy.OutputField()
            confidence: float = dspy.OutputField()
        
        processor = dspy.ChainOfThought(CloudOptimizedProcessor)
        
        with usage_tracker.track() as tracker:
            result = processor(
                request=str(request_data),
                cloud_provider=self.cloud
            )
        
        # Send metrics to cloud provider
        await self.provider.send_metrics('request_processed', 1)
        await self.provider.send_metrics('processing_cost', tracker.cost)
        
        # Log event
        await self.provider.log_event({
            'event_type': 'request_processed',
            'cloud': self.cloud,
            'cost': tracker.cost,
            'tokens': tracker.total_tokens,
            'confidence': result.confidence
        })
        
        return {
            'result': result.result,
            'confidence': result.confidence,
            'cloud': self.cloud,
            'usage': {
                'cost': tracker.cost,
                'tokens': tracker.total_tokens
            }
        }
```

### Serverless DSPy Patterns

**Cross-Platform Serverless Framework**
```python
# serverless_dspy.py
import json
import asyncio
from typing import Dict, Any, Optional
import dspy
from dspy.utils import usage_tracker

class ServerlessDSPyHandler:
    """Universal serverless handler for multiple cloud platforms"""
    
    def __init__(self):
        self.dspy_initialized = False
        self.lm = None
    
    def _detect_platform(self, event: Dict[str, Any]) -> str:
        """Detect serverless platform from event structure"""
        if 'Records' in event and 'awsRegion' in str(event):
            return 'aws_lambda'
        elif 'httpMethod' in event and 'requestContext' in event:
            return 'aws_apigateway'
        elif 'data' in event and '@type' in event:
            return 'gcp_functions'
        elif 'req' in event or 'request' in event:
            return 'azure_functions'
        else:
            return 'unknown'
    
    async def _initialize_dspy(self, platform: str):
        """Initialize DSPy based on platform"""
        if self.dspy_initialized:
            return
        
        if platform.startswith('aws'):
            # Use environment variables or Parameter Store
            api_key = os.environ.get('OPENAI_API_KEY')
            if not api_key:
                import boto3
                ssm = boto3.client('ssm')
                response = ssm.get_parameter(
                    Name='/dspy/openai/api_key',
                    WithDecryption=True
                )
                api_key = response['Parameter']['Value']
        
        elif platform.startswith('gcp'):
            # Use Secret Manager
            from google.cloud import secretmanager
            client = secretmanager.SecretManagerServiceClient()
            name = f"projects/{os.environ['GCP_PROJECT']}/secrets/openai-api-key/versions/latest"
            response = client.access_secret_version(request={"name": name})
            api_key = response.payload.data.decode("UTF-8")
        
        elif platform.startswith('azure'):
            # Use Key Vault
            from azure.identity import DefaultAzureCredential
            from azure.keyvault.secrets import SecretClient
            credential = DefaultAzureCredential()
            vault_url = f"https://{os.environ['KEY_VAULT_NAME']}.vault.azure.net/"
            client = SecretClient(vault_url=vault_url, credential=credential)
            api_key = client.get_secret("openai-api-key").value
        
        # Initialize DSPy with optimized settings for serverless
        self.lm = dspy.LM(
            'openai/gpt-4o-mini',  # Faster for serverless
            api_key=api_key,
            max_tokens=1000,  # Limit for faster response
            temperature=0.1   # More deterministic
        )
        dspy.configure(lm=self.lm)
        self.dspy_initialized = True
    
    async def process_quick_query(self, query: str, context: str = "") -> Dict[str, Any]:
        """Optimized processing for serverless constraints"""
        class ServerlessProcessor(dspy.Signature):
            """Fast processing optimized for serverless execution"""
            query: str = dspy.InputField()
            context: str = dspy.InputField()
            response: str = dspy.OutputField()
        
        # Use ChainOfThought for better results with minimal overhead
        processor = dspy.ChainOfThought(ServerlessProcessor)
        
        with usage_tracker.track() as tracker:
            result = processor(query=query, context=context)
        
        return {
            'response': result.response,
            'cost': tracker.cost,
            'tokens': tracker.total_tokens,
            'model': 'gpt-4o-mini'
        }
    
    # AWS Lambda Handler
    def lambda_handler(self, event, context):
        """AWS Lambda entry point"""
        platform = self._detect_platform(event)
        
        async def process():
            await self._initialize_dspy(platform)
            
            # Extract query from different event types
            if 'body' in event:
                body = json.loads(event['body']) if isinstance(event['body'], str) else event['body']
                query = body.get('query', '')
                ctx = body.get('context', '')
            else:
                query = event.get('query', '')
                ctx = event.get('context', '')
            
            result = await self.process_quick_query(query, ctx)
            
            return {
                'statusCode': 200,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'body': json.dumps(result)
            }
        
        # Run async function in Lambda
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(process())
        finally:
            loop.close()
    
    # Google Cloud Functions Handler
    def gcp_handler(self, request):
        """Google Cloud Functions entry point"""
        import functions_framework
        
        async def process():
            await self._initialize_dspy('gcp_functions')
            
            request_json = request.get_json(silent=True)
            if request_json:
                query = request_json.get('query', '')
                context = request_json.get('context', '')
            else:
                query = request.args.get('query', '')
                context = request.args.get('context', '')
            
            result = await self.process_quick_query(query, context)
            return result
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(process())
        finally:
            loop.close()
    
    # Azure Functions Handler
    def azure_handler(self, req):
        """Azure Functions entry point"""
        import azure.functions as func
        
        async def process():
            await self._initialize_dspy('azure_functions')
            
            try:
                req_body = req.get_json()
                query = req_body.get('query', '')
                context = req_body.get('context', '')
            except ValueError:
                query = req.params.get('query', '')
                context = req.params.get('context', '')
            
            result = await self.process_quick_query(query, context)
            
            return func.HttpResponse(
                json.dumps(result),
                status_code=200,
                headers={'Content-Type': 'application/json'}
            )
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(process())
        finally:
            loop.close()

# Global handler instance for reuse
handler = ServerlessDSPyHandler()

# Export handlers for each platform
lambda_handler = handler.lambda_handler
gcp_handler = handler.gcp_handler
azure_handler = handler.azure_handler
```

## Optimization Strategies

### Cold Start Optimization

**Serverless Cold Start Minimization**
```python
# cold_start_optimizer.py
import os
import pickle
import hashlib
from typing import Dict, Any
import dspy

class ColdStartOptimizer:
    """Optimize DSPy for minimal cold start times"""
    
    def __init__(self):
        self.cache_dir = '/tmp/dspy_cache'
        os.makedirs(self.cache_dir, exist_ok=True)
        self.signature_cache = {}
    
    def get_cached_signature(self, signature_class, cache_key: str = None):
        """Cache compiled signatures to reduce cold start time"""
        if not cache_key:
            cache_key = hashlib.md5(
                str(signature_class.__name__).encode()
            ).hexdigest()
        
        cache_path = f"{self.cache_dir}/{cache_key}.pkl"
        
        # Try to load from cache
        if os.path.exists(cache_path) and cache_key in self.signature_cache:
            return self.signature_cache[cache_key]
        
        # Create and cache new signature
        try:
            signature = dspy.ChainOfThought(signature_class)
            
            # Cache in memory
            self.signature_cache[cache_key] = signature
            
            # Cache to disk for next cold start
            try:
                with open(cache_path, 'wb') as f:
                    pickle.dump(signature, f)
            except Exception as e:
                # Ignore disk cache errors
                pass
            
            return signature
        except Exception as e:
            # Fallback to uncached signature
            return dspy.ChainOfThought(signature_class)
    
    def precompile_signatures(self, signature_classes: list):
        """Precompile common signatures during initialization"""
        for sig_class in signature_classes:
            self.get_cached_signature(sig_class)
    
    def optimize_lm_config(self) -> dict:
        """Optimal LM configuration for serverless"""
        return {
            'max_tokens': 500,  # Faster generation
            'temperature': 0.1,  # More deterministic
            'timeout': 30,      # Prevent hanging
            'max_retries': 1    # Minimize retry overhead
        }

# Usage in serverless function
optimizer = ColdStartOptimizer()

# Predefine common signatures
class QuickResponse(dspy.Signature):
    """Generate quick response for common queries"""
    query: str = dspy.InputField()
    response: str = dspy.OutputField()

class DataSummary(dspy.Signature):
    """Summarize data quickly"""
    data: str = dspy.InputField()
    summary: str = dspy.OutputField()

# Precompile during module load
optimizer.precompile_signatures([QuickResponse, DataSummary])

def serverless_handler(event, context):
    """Optimized serverless handler"""
    # Fast DSPy initialization
    if not hasattr(serverless_handler, 'initialized'):
        lm_config = optimizer.optimize_lm_config()
        lm = dspy.LM('openai/gpt-4o-mini', **lm_config)
        dspy.configure(lm=lm)
        serverless_handler.initialized = True
    
    # Use cached signature
    processor = optimizer.get_cached_signature(QuickResponse)
    
    query = json.loads(event['body'])['query']
    result = processor(query=query)
    
    return {
        'statusCode': 200,
        'body': json.dumps({'response': result.response})
    }
```

### Auto-Scaling Configuration

**Cloud-Native Auto-Scaling**
```python
# autoscaling_config.py
import asyncio
import time
from collections import defaultdict
import dspy

class DSPyAutoScaler:
    """Intelligent auto-scaling for DSPy services"""
    
    def __init__(self):
        self.request_metrics = defaultdict(list)
        self.performance_metrics = defaultdict(list)
        self.scaling_decisions = []
    
    async def track_request(self, request_id: str, start_time: float, 
                           end_time: float, cost: float, tokens: int):
        """Track request performance for scaling decisions"""
        duration = end_time - start_time
        
        self.request_metrics['duration'].append(duration)
        self.request_metrics['cost'].append(cost)
        self.request_metrics['tokens'].append(tokens)
        self.request_metrics['timestamp'].append(time.time())
        
        # Keep only last 1000 requests
        for key in self.request_metrics:
            if len(self.request_metrics[key]) > 1000:
                self.request_metrics[key] = self.request_metrics[key][-1000:]
    
    def get_scaling_recommendation(self) -> dict:
        """Analyze metrics and recommend scaling action"""
        if not self.request_metrics['duration']:
            return {'action': 'maintain', 'reason': 'insufficient_data'}
        
        # Calculate recent metrics (last 5 minutes)
        current_time = time.time()
        recent_mask = [
            t > current_time - 300 
            for t in self.request_metrics['timestamp']
        ]
        
        if not any(recent_mask):
            return {'action': 'maintain', 'reason': 'no_recent_requests'}
        
        recent_durations = [
            d for d, is_recent in zip(self.request_metrics['duration'], recent_mask)
            if is_recent
        ]
        
        avg_duration = sum(recent_durations) / len(recent_durations)
        max_duration = max(recent_durations)
        request_rate = len(recent_durations) / 5.0  # requests per minute
        
        # Scaling logic
        if avg_duration > 10.0 or max_duration > 30.0:
            return {
                'action': 'scale_up',
                'reason': 'high_latency',
                'metrics': {
                    'avg_duration': avg_duration,
                    'max_duration': max_duration,
                    'request_rate': request_rate
                }
            }
        elif request_rate > 100:  # More than 100 requests per minute
            return {
                'action': 'scale_up',
                'reason': 'high_traffic',
                'metrics': {
                    'request_rate': request_rate,
                    'avg_duration': avg_duration
                }
            }
        elif request_rate < 10 and avg_duration < 2.0:  # Low traffic, good performance
            return {
                'action': 'scale_down',
                'reason': 'low_utilization',
                'metrics': {
                    'request_rate': request_rate,
                    'avg_duration': avg_duration
                }
            }
        else:
            return {
                'action': 'maintain',
                'reason': 'optimal_performance',
                'metrics': {
                    'request_rate': request_rate,
                    'avg_duration': avg_duration
                }
            }
    
    async def apply_scaling_decision(self, cloud_provider: str, decision: dict):
        """Apply scaling decision to cloud infrastructure"""
        if decision['action'] == 'maintain':
            return
        
        if cloud_provider == 'aws':
            await self._scale_aws(decision)
        elif cloud_provider == 'gcp':
            await self._scale_gcp(decision)
        elif cloud_provider == 'azure':
            await self._scale_azure(decision)
    
    async def _scale_aws(self, decision: dict):
        """Scale AWS ECS or Lambda"""
        import boto3
        
        if decision['action'] == 'scale_up':
            # Increase ECS desired count or Lambda concurrency
            ecs = boto3.client('ecs')
            ecs.update_service(
                cluster='dspy-cluster',
                service='dspy-service',
                desiredCount=5  # Scale up
            )
        elif decision['action'] == 'scale_down':
            ecs = boto3.client('ecs')
            ecs.update_service(
                cluster='dspy-cluster',
                service='dspy-service',
                desiredCount=2  # Scale down
            )
    
    async def _scale_gcp(self, decision: dict):
        """Scale GKE or Cloud Run"""
        # GKE HPA or Cloud Run scaling
        pass
    
    async def _scale_azure(self, decision: dict):
        """Scale AKS or Container Apps"""
        # AKS HPA or Container Apps scaling
        pass

# Kubernetes HPA configuration
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: dspy-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: dspy-service
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: dspy_request_duration
      target:
        type: AverageValue
        averageValue: "5000m"  # 5 seconds
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
```

## Speed Tips

1. **Use smaller models for serverless** - gpt-4o-mini for faster cold starts
2. **Cache compiled signatures** - Precompile common DSPy patterns
3. **Optimize container images** - Use multi-stage builds and minimal base images
4. **Configure auto-scaling properly** - Set appropriate CPU/memory thresholds
5. **Use managed services** - Leverage cloud-native databases and caching
6. **Implement circuit breakers** - Prevent cascade failures in distributed systems
7. **Use CDN for static assets** - Reduce global latency for web interfaces
8. **Monitor cold start metrics** - Track and optimize initialization times

## Common Pitfalls

1. **Ignoring cold starts** - Not optimizing for serverless initialization time
2. **Over-provisioning resources** - Wasting money on unnecessary compute capacity
3. **Missing health checks** - Not implementing proper liveness/readiness probes
4. **Inadequate monitoring** - Not tracking performance metrics and costs
5. **Poor secret management** - Hardcoding credentials instead of using cloud services
6. **No disaster recovery** - Not planning for multi-region failover
7. **Ignoring compliance** - Not considering data residency and privacy requirements
8. **Scaling too aggressively** - Causing unnecessary costs and potential instability

## Best Practices

1. **Use infrastructure as code** - Terraform, CloudFormation, or Bicep for reproducible deployments
2. **Implement proper logging** - Structured logging with correlation IDs
3. **Set up monitoring dashboards** - Track key metrics across all services
4. **Use blue-green deployments** - Zero-downtime deployments for production
5. **Implement proper security** - Network policies, RBAC, and secret management
6. **Plan for multi-region** - Design for geographical distribution and failover
7. **Monitor costs actively** - Set up billing alerts and cost optimization
8. **Regular disaster recovery testing** - Validate backup and recovery procedures

## References

- [AWS Well-Architected Framework](https://aws.amazon.com/architecture/well-architected/)
- [Google Cloud Architecture Center](https://cloud.google.com/architecture)
- [Azure Architecture Center](https://learn.microsoft.com/en-us/azure/architecture/)
- [Kubernetes Best Practices](https://kubernetes.io/docs/concepts/configuration/overview/)
- [DSPy Documentation](https://dspy-docs.vercel.app/)