---
description: Usage Analytics - Usage tracking and cost optimization for DSPy production systems based on usage_tracker.py analysis
alwaysApply: false
---

> You are an expert in DSPy 3.0.1 usage analytics and cost optimization for production environments.

## Usage Analytics Architecture

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   DSPy Client   │ => │  Usage Tracker  │ => │  Cost Analytics │
│   Application   │    │  (Track Calls)  │    │   (Calculate)   │
└─────────────────┘    └─────────────────┘    └─────────────────┘
          │                       │                       │
          v                       v                       v
┌─────────────────────────────────────────────────────────────────┐
│                   Usage Tracking Layer                         │
├─────────────────┬─────────────────┬─────────────────────────────┤
│   Token Count   │   API Calls     │     Response Times          │
│   (Input/Output)│   (Per Model)   │     (Latency Track)         │
└─────────────────┴─────────────────┴─────────────────────────────┘
          │                       │                       │
          v                       v                       v
┌─────────────────┬─────────────────┬─────────────────────────────┐
│   Cost Calc     │   Usage Stats   │     Optimization            │
│   (Per Request) │   (Aggregated)  │     (Recommendations)       │
└─────────────────┴─────────────────┴─────────────────────────────┘
          │                       │                       │
          v                       v                       v
┌─────────────────────────────────────────────────────────────────┐
│                 Analytics & Reporting                          │
├─────────────────┬─────────────────┬─────────────────────────────┤
│   Dashboards    │   Alerts        │     Budgets & Limits        │
│   (Grafana/UI)  │   (Thresholds)  │     (Cost Controls)         │
└─────────────────┴─────────────────┴─────────────────────────────┘
```

## Instant Usage Tracking Patterns

### Quick Start Usage Tracking

```python
# quick_usage.py - Basic usage tracking with DSPy
import dspy
from dspy.utils.usage_tracker import track_usage
from typing import Dict, Any
import json
import time

def basic_usage_tracking():
    """Basic usage tracking pattern"""

    # Configure DSPy
    lm = dspy.LM("openai/gpt-4o-mini")
    dspy.configure(lm=lm)

    # Create program
    program = dspy.ChainOfThought("question -> answer")

    # Track usage
    with track_usage() as tracker:
        # Make multiple predictions
        questions = [
            "What is machine learning?",
            "Explain neural networks",
            "How does backpropagation work?"
        ]

        for question in questions:
            result = program(question=question)
            print(f"Q: {question}")
            print(f"A: {result.answer[:100]}...")
            print()

        # Get usage statistics
        usage_stats = tracker.get_total_tokens()
        print("Usage Statistics:")
        print(json.dumps(usage_stats, indent=2))

        return usage_stats

# Run basic tracking
usage_stats = basic_usage_tracking()
```

### Enterprise Usage Analytics

```python
# enterprise_usage.py - Comprehensive usage analytics system
import dspy
from dspy.utils.usage_tracker import UsageTracker
from typing import Dict, Any, List, Optional
import json
import time
import logging
from datetime import datetime, timedelta
from dataclasses import dataclass
import pandas as pd
from collections import defaultdict
import asyncio

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class CostConfig:
    """Cost configuration for different LM providers"""
    provider: str
    model: str
    input_cost_per_1k: float  # Cost per 1000 input tokens
    output_cost_per_1k: float  # Cost per 1000 output tokens

class EnterpriseUsageAnalytics:
    """Enterprise-grade usage analytics and cost tracking"""

    def __init__(self):
        self.usage_history = []
        self.cost_configs = self._initialize_cost_configs()
        self.session_start = datetime.now()

        # Real-time tracking
        self.current_tracker = UsageTracker()
        self.daily_usage = defaultdict(lambda: defaultdict(float))
        self.hourly_usage = defaultdict(lambda: defaultdict(float))

    def _initialize_cost_configs(self) -> Dict[str, CostConfig]:
        """Initialize cost configurations for different models"""
        return {
            "openai/gpt-4o-mini": CostConfig("openai", "gpt-4o-mini", 0.000150, 0.000600),
            "openai/gpt-4o": CostConfig("openai", "gpt-4o", 0.0050, 0.0150),
            "openai/gpt-3.5-turbo": CostConfig("openai", "gpt-3.5-turbo", 0.0015, 0.002),
            "anthropic/claude-3-haiku-20240307": CostConfig("anthropic", "claude-3-haiku", 0.00025, 0.00125),
            "anthropic/claude-3-sonnet-20240229": CostConfig("anthropic", "claude-3-sonnet", 0.003, 0.015),
            "local/llama-3.1-8b": CostConfig("local", "llama-3.1-8b", 0.0, 0.0),  # Free local inference
        }

    def track_usage_with_context(self,
                                operation_name: str,
                                user_id: str = None,
                                session_id: str = None,
                                metadata: Dict[str, Any] = None):
        """Context manager for tracking usage with additional metadata"""

        class UsageContext:
            def __init__(self, analytics, operation_name, user_id, session_id, metadata):
                self.analytics = analytics
                self.operation_name = operation_name
                self.user_id = user_id
                self.session_id = session_id
                self.metadata = metadata or {}
                self.start_time = None
                self.tracker = None

            def __enter__(self):
                self.start_time = time.time()
                self.tracker = UsageTracker()

                # Set tracker in DSPy settings
                from dspy.dsp.utils.settings import settings
                settings.usage_tracker = self.tracker

                return self.tracker

            def __exit__(self, exc_type, exc_val, exc_tb):
                end_time = time.time()
                duration = end_time - self.start_time

                # Get usage data
                usage_data = self.tracker.get_total_tokens()

                # Calculate costs
                costs = self.analytics.calculate_costs(usage_data)

                # Create usage record
                record = {
                    "timestamp": datetime.now().isoformat(),
                    "operation": self.operation_name,
                    "user_id": self.user_id,
                    "session_id": self.session_id,
                    "duration_seconds": duration,
                    "usage_data": usage_data,
                    "costs": costs,
                    "metadata": self.metadata,
                    "success": exc_type is None
                }

                # Store record
                self.analytics.add_usage_record(record)

                # Clear tracker
                settings.usage_tracker = None

        return UsageContext(self, operation_name, user_id, session_id, metadata)

    def calculate_costs(self, usage_data: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
        """Calculate costs from usage data"""

        total_cost = 0.0
        cost_breakdown = {}

        for model_name, usage_entries in usage_data.items():
            if model_name not in self.cost_configs:
                continue

            config = self.cost_configs[model_name]
            model_cost = 0.0

            for entry in usage_entries:
                # Get token counts
                input_tokens = entry.get("prompt_tokens", 0)
                output_tokens = entry.get("completion_tokens", 0)

                # Calculate costs
                input_cost = (input_tokens / 1000) * config.input_cost_per_1k
                output_cost = (output_tokens / 1000) * config.output_cost_per_1k

                model_cost += input_cost + output_cost

            cost_breakdown[model_name] = {
                "total_cost": model_cost,
                "provider": config.provider,
                "model": config.model,
                "total_input_tokens": sum(e.get("prompt_tokens", 0) for e in usage_entries),
                "total_output_tokens": sum(e.get("completion_tokens", 0) for e in usage_entries),
                "num_requests": len(usage_entries)
            }

            total_cost += model_cost

        return {
            "total_cost": total_cost,
            "breakdown": cost_breakdown,
            "currency": "USD"
        }

    def add_usage_record(self, record: Dict[str, Any]):
        """Add usage record to analytics"""

        self.usage_history.append(record)

        # Update real-time tracking
        timestamp = datetime.fromisoformat(record["timestamp"])
        date_key = timestamp.strftime("%Y-%m-%d")
        hour_key = timestamp.strftime("%Y-%m-%d %H:00")

        for model_name, cost_info in record["costs"]["breakdown"].items():
            self.daily_usage[date_key][model_name] += cost_info["total_cost"]
            self.hourly_usage[hour_key][model_name] += cost_info["total_cost"]

        # Log significant costs
        if record["costs"]["total_cost"] > 1.0:  # Alert for expensive operations
            logger.warning(f"High cost operation: {record['operation']} - ${record['costs']['total_cost']:.4f}")

    def get_usage_summary(self,
                         start_date: datetime = None,
                         end_date: datetime = None,
                         group_by: str = "day") -> Dict[str, Any]:
        """Get comprehensive usage summary"""

        if not start_date:
            start_date = self.session_start
        if not end_date:
            end_date = datetime.now()

        # Filter records by date range
        filtered_records = [
            record for record in self.usage_history
            if start_date <= datetime.fromisoformat(record["timestamp"]) <= end_date
        ]

        if not filtered_records:
            return {"error": "No usage data found for date range"}

        # Aggregate statistics
        total_cost = sum(record["costs"]["total_cost"] for record in filtered_records)
        total_operations = len(filtered_records)
        successful_operations = sum(1 for record in filtered_records if record["success"])

        # Model usage breakdown
        model_stats = defaultdict(lambda: {
            "requests": 0,
            "total_cost": 0.0,
            "input_tokens": 0,
            "output_tokens": 0
        })

        for record in filtered_records:
            for model_name, cost_info in record["costs"]["breakdown"].items():
                stats = model_stats[model_name]
                stats["requests"] += cost_info["num_requests"]
                stats["total_cost"] += cost_info["total_cost"]
                stats["input_tokens"] += cost_info["total_input_tokens"]
                stats["output_tokens"] += cost_info["total_output_tokens"]

        # User analysis
        user_stats = defaultdict(lambda: {"operations": 0, "cost": 0.0})
        for record in filtered_records:
            user_id = record.get("user_id", "unknown")
            user_stats[user_id]["operations"] += 1
            user_stats[user_id]["cost"] += record["costs"]["total_cost"]

        # Time-based analysis
        time_series = self._generate_time_series(filtered_records, group_by)

        return {
            "summary": {
                "date_range": {
                    "start": start_date.isoformat(),
                    "end": end_date.isoformat()
                },
                "total_cost": total_cost,
                "total_operations": total_operations,
                "success_rate": successful_operations / total_operations if total_operations > 0 else 0,
                "avg_cost_per_operation": total_cost / total_operations if total_operations > 0 else 0
            },
            "model_breakdown": dict(model_stats),
            "user_analysis": dict(user_stats),
            "time_series": time_series,
            "top_expensive_operations": sorted(
                filtered_records,
                key=lambda x: x["costs"]["total_cost"],
                reverse=True
            )[:10]
        }

    def _generate_time_series(self, records: List[Dict], group_by: str) -> Dict[str, Any]:
        """Generate time series data for visualization"""

        time_series = defaultdict(lambda: {"cost": 0.0, "operations": 0})

        for record in records:
            timestamp = datetime.fromisoformat(record["timestamp"])

            if group_by == "hour":
                key = timestamp.strftime("%Y-%m-%d %H:00")
            elif group_by == "day":
                key = timestamp.strftime("%Y-%m-%d")
            elif group_by == "week":
                # Get Monday of the week
                monday = timestamp - timedelta(days=timestamp.weekday())
                key = monday.strftime("%Y-%m-%d")
            else:
                key = timestamp.strftime("%Y-%m-%d")

            time_series[key]["cost"] += record["costs"]["total_cost"]
            time_series[key]["operations"] += 1

        return dict(time_series)

    def set_cost_alerts(self,
                       daily_limit: float = 100.0,
                       hourly_limit: float = 10.0,
                       operation_limit: float = 5.0):
        """Set up cost alerting thresholds"""

        self.cost_alerts = {
            "daily_limit": daily_limit,
            "hourly_limit": hourly_limit,
            "operation_limit": operation_limit
        }

    def check_cost_alerts(self, current_record: Dict[str, Any]) -> List[str]:
        """Check if any cost alerts are triggered"""

        alerts = []

        if not hasattr(self, 'cost_alerts'):
            return alerts

        # Check operation cost
        operation_cost = current_record["costs"]["total_cost"]
        if operation_cost > self.cost_alerts["operation_limit"]:
            alerts.append(f"High operation cost: ${operation_cost:.4f} > ${self.cost_alerts['operation_limit']}")

        # Check daily cost
        today = datetime.now().strftime("%Y-%m-%d")
        daily_cost = sum(self.daily_usage[today].values())
        if daily_cost > self.cost_alerts["daily_limit"]:
            alerts.append(f"Daily cost limit exceeded: ${daily_cost:.2f} > ${self.cost_alerts['daily_limit']}")

        # Check hourly cost
        current_hour = datetime.now().strftime("%Y-%m-%d %H:00")
        hourly_cost = sum(self.hourly_usage[current_hour].values())
        if hourly_cost > self.cost_alerts["hourly_limit"]:
            alerts.append(f"Hourly cost limit exceeded: ${hourly_cost:.2f} > ${self.cost_alerts['hourly_limit']}")

        return alerts

    def export_usage_data(self, format: str = "json", filename: str = None) -> str:
        """Export usage data to file"""

        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"dspy_usage_{timestamp}.{format}"

        if format == "json":
            with open(filename, 'w') as f:
                json.dump({
                    "session_info": {
                        "start_time": self.session_start.isoformat(),
                        "export_time": datetime.now().isoformat(),
                        "total_records": len(self.usage_history)
                    },
                    "usage_records": self.usage_history,
                    "cost_configs": {
                        name: {
                            "provider": config.provider,
                            "model": config.model,
                            "input_cost_per_1k": config.input_cost_per_1k,
                            "output_cost_per_1k": config.output_cost_per_1k
                        }
                        for name, config in self.cost_configs.items()
                    }
                }, f, indent=2)

        elif format == "csv":
            # Flatten records for CSV
            flat_records = []
            for record in self.usage_history:
                base_record = {
                    "timestamp": record["timestamp"],
                    "operation": record["operation"],
                    "user_id": record.get("user_id", ""),
                    "session_id": record.get("session_id", ""),
                    "duration_seconds": record["duration_seconds"],
                    "total_cost": record["costs"]["total_cost"],
                    "success": record["success"]
                }

                # Add model-specific data
                for model_name, cost_info in record["costs"]["breakdown"].items():
                    model_record = base_record.copy()
                    model_record.update({
                        "model_name": model_name,
                        "provider": cost_info["provider"],
                        "model_cost": cost_info["total_cost"],
                        "input_tokens": cost_info["total_input_tokens"],
                        "output_tokens": cost_info["total_output_tokens"],
                        "num_requests": cost_info["num_requests"]
                    })
                    flat_records.append(model_record)

            df = pd.DataFrame(flat_records)
            df.to_csv(filename, index=False)

        logger.info(f"Usage data exported to {filename}")
        return filename

# Production usage analytics example
def production_analytics_example():
    """Example of production usage analytics"""

    # Initialize analytics
    analytics = EnterpriseUsageAnalytics()
    analytics.set_cost_alerts(daily_limit=50.0, hourly_limit=5.0, operation_limit=2.0)

    # Configure DSPy with multiple models
    models = {
        "fast": dspy.LM("openai/gpt-4o-mini"),
        "quality": dspy.LM("openai/gpt-4o"),
        "local": dspy.LM("local/llama-3.1-8b") if False else dspy.LM("openai/gpt-4o-mini")  # Fallback
    }

    # Create programs
    programs = {
        name: dspy.ChainOfThought("question -> answer")
        for name in models.keys()
    }

    # Simulate production usage
    operations = [
        ("user_onboarding", "fast", "What is our product?"),
        ("content_generation", "quality", "Write a detailed product description"),
        ("support_query", "fast", "How do I reset my password?"),
        ("research_task", "quality", "Analyze market trends in AI"),
        ("batch_processing", "local", "Summarize this text")
    ]

    for i, (operation_type, model_type, question) in enumerate(operations):
        # Configure DSPy for specific model
        dspy.configure(lm=models[model_type])

        # Track usage with context
        with analytics.track_usage_with_context(
            operation_name=operation_type,
            user_id=f"user_{i % 3 + 1}",  # Simulate 3 users
            session_id=f"session_{i // 2 + 1}",  # Group operations into sessions
            metadata={"model_type": model_type, "question_length": len(question)}
        ) as tracker:

            program = programs[model_type]
            result = program(question=question)

            print(f"Operation: {operation_type} (Model: {model_type})")
            print(f"Q: {question}")
            print(f"A: {result.answer[:100]}...")

            # Check for alerts
            if analytics.usage_history:
                alerts = analytics.check_cost_alerts(analytics.usage_history[-1])
                if alerts:
                    for alert in alerts:
                        logger.warning(f"COST ALERT: {alert}")
            print()

    # Get comprehensive summary
    summary = analytics.get_usage_summary()
    print("\n" + "="*50)
    print("USAGE ANALYTICS SUMMARY")
    print("="*50)
    print(json.dumps(summary["summary"], indent=2))

    print("\nMODEL BREAKDOWN:")
    for model, stats in summary["model_breakdown"].items():
        print(f"  {model}: ${stats['total_cost']:.4f} ({stats['requests']} requests)")

    print("\nUSER ANALYSIS:")
    for user, stats in summary["user_analysis"].items():
        print(f"  {user}: ${stats['cost']:.4f} ({stats['operations']} operations)")

    # Export data
    filename = analytics.export_usage_data(format="json")
    print(f"\nUsage data exported to: {filename}")

# Usage monitoring for async applications
class AsyncUsageMonitor:
    """Async-compatible usage monitoring"""

    def __init__(self):
        self.analytics = EnterpriseUsageAnalytics()
        self.monitoring_active = False

    async def start_monitoring(self, check_interval: int = 60):
        """Start background monitoring task"""

        self.monitoring_active = True

        async def monitoring_loop():
            while self.monitoring_active:
                try:
                    # Check for cost alerts
                    current_time = datetime.now()
                    daily_cost = sum(self.analytics.daily_usage[current_time.strftime("%Y-%m-%d")].values())

                    if daily_cost > 100.0:  # Daily threshold
                        logger.warning(f"Daily cost threshold exceeded: ${daily_cost:.2f}")

                    await asyncio.sleep(check_interval)

                except Exception as e:
                    logger.error(f"Monitoring error: {e}")
                    await asyncio.sleep(check_interval)

        # Start monitoring task
        asyncio.create_task(monitoring_loop())
        logger.info("Async usage monitoring started")

    def stop_monitoring(self):
        """Stop background monitoring"""
        self.monitoring_active = False
        logger.info("Async usage monitoring stopped")

# Run production example
if __name__ == "__main__":
    production_analytics_example()
```

## Core Usage Analytics Patterns

### Real-time Cost Monitoring

```python
# Real-time cost monitoring with alerts
import dspy
from dspy.utils.usage_tracker import track_usage
import threading
import time
from typing import Callable, Dict, Any

class RealTimeCostMonitor:
    """Real-time cost monitoring with automatic alerts"""

    def __init__(self, alert_callback: Callable[[str], None] = None):
        self.alert_callback = alert_callback or print
        self.running_cost = 0.0
        self.cost_per_model = {
            "openai/gpt-4o-mini": 0.0002,  # Rough estimate per request
            "openai/gpt-4o": 0.01,
            "anthropic/claude-3-haiku-20240307": 0.0005
        }

        self.monitoring = False
        self.monitor_thread = None

    def start_monitoring(self, check_interval: int = 10):
        """Start monitoring in background thread"""

        if self.monitoring:
            return

        self.monitoring = True

        def monitor_loop():
            while self.monitoring:
                self._check_cost_thresholds()
                time.sleep(check_interval)

        self.monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        self.monitor_thread.start()

    def stop_monitoring(self):
        """Stop background monitoring"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()

    def track_operation(self, operation_name: str, model_name: str):
        """Context manager for tracking individual operations"""

        class OperationTracker:
            def __init__(self, monitor, operation_name, model_name):
                self.monitor = monitor
                self.operation_name = operation_name
                self.model_name = model_name
                self.start_cost = monitor.running_cost

            def __enter__(self):
                return track_usage()

            def __exit__(self, exc_type, exc_val, exc_tb):
                # Estimate operation cost
                estimated_cost = self.monitor.cost_per_model.get(self.model_name, 0.001)
                self.monitor.running_cost += estimated_cost

                print(f"Operation: {self.operation_name} (+${estimated_cost:.4f})")
                print(f"Total cost: ${self.monitor.running_cost:.4f}")

        return OperationTracker(self, operation_name, model_name)

    def _check_cost_thresholds(self):
        """Check cost thresholds and trigger alerts"""

        thresholds = [
            (10.0, "Daily budget warning: $10 reached"),
            (25.0, "Daily budget alert: $25 reached"),
            (50.0, "CRITICAL: Daily budget limit $50 reached")
        ]

        for threshold, message in thresholds:
            if self.running_cost >= threshold and not hasattr(self, f'_alerted_{threshold}'):
                self.alert_callback(f"COST ALERT: {message}")
                setattr(self, f'_alerted_{threshold}', True)

# Usage
monitor = RealTimeCostMonitor()
monitor.start_monitoring(check_interval=5)

# Track operations
lm = dspy.LM("openai/gpt-4o-mini")
dspy.configure(lm=lm)
program = dspy.ChainOfThought("question -> answer")

with monitor.track_operation("user_query", "openai/gpt-4o-mini") as tracker:
    result = program(question="What is deep learning?")
```

### Usage Optimization Recommendations

```python
# Automatic usage optimization recommendations
import dspy
from typing import List, Dict, Any, Tuple
import statistics

class UsageOptimizer:
    """Analyze usage patterns and provide optimization recommendations"""

    def __init__(self):
        self.usage_data = []
        self.model_performance = {}

    def analyze_usage_patterns(self, usage_records: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze usage patterns for optimization opportunities"""

        analysis = {
            "total_cost": 0,
            "model_usage": {},
            "optimization_opportunities": [],
            "recommendations": []
        }

        # Aggregate by model
        for record in usage_records:
            analysis["total_cost"] += record["costs"]["total_cost"]

            for model_name, cost_info in record["costs"]["breakdown"].items():
                if model_name not in analysis["model_usage"]:
                    analysis["model_usage"][model_name] = {
                        "total_cost": 0,
                        "requests": 0,
                        "avg_cost_per_request": 0,
                        "input_tokens": 0,
                        "output_tokens": 0
                    }

                stats = analysis["model_usage"][model_name]
                stats["total_cost"] += cost_info["total_cost"]
                stats["requests"] += cost_info["num_requests"]
                stats["input_tokens"] += cost_info["total_input_tokens"]
                stats["output_tokens"] += cost_info["total_output_tokens"]

        # Calculate averages
        for model_name, stats in analysis["model_usage"].items():
            if stats["requests"] > 0:
                stats["avg_cost_per_request"] = stats["total_cost"] / stats["requests"]

        # Generate recommendations
        recommendations = self._generate_recommendations(analysis)
        analysis["recommendations"] = recommendations

        return analysis

    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """Generate optimization recommendations based on analysis"""

        recommendations = []

        # Check for expensive models
        expensive_models = {
            name: stats for name, stats in analysis["model_usage"].items()
            if stats["avg_cost_per_request"] > 0.01  # > 1 cent per request
        }

        if expensive_models:
            recommendations.append(
                f"Consider using cheaper models for simple tasks. "
                f"Models with high per-request costs: {list(expensive_models.keys())}"
            )

        # Check for high token usage
        high_token_models = {
            name: stats for name, stats in analysis["model_usage"].items()
            if (stats["input_tokens"] + stats["output_tokens"]) / stats["requests"] > 2000
        }

        if high_token_models:
            recommendations.append(
                f"High token usage detected. Consider optimizing prompts or using summarization for: {list(high_token_models.keys())}"
            )

        # Check for caching opportunities
        total_requests = sum(stats["requests"] for stats in analysis["model_usage"].values())
        if total_requests > 100:
            recommendations.append(
                "High request volume detected. Implement caching for frequently asked questions to reduce costs."
            )

        # Model consolidation
        if len(analysis["model_usage"]) > 3:
            recommendations.append(
                f"Using {len(analysis['model_usage'])} different models. "
                "Consider consolidating to fewer models to simplify operations and potentially reduce costs."
            )

        return recommendations

    def suggest_model_alternatives(self, current_model: str, use_case: str) -> List[Tuple[str, str, float]]:
        """Suggest alternative models for cost optimization"""

        # Model alternatives with cost estimates (simplified)
        alternatives = {
            "openai/gpt-4o": [
                ("openai/gpt-4o-mini", "For most tasks, similar quality at 10x lower cost", 0.9),
                ("anthropic/claude-3-haiku-20240307", "Fast and cheap alternative", 0.8),
            ],
            "openai/gpt-4o-mini": [
                ("local/llama-3.1-8b", "Free local inference (if available)", 1.0),
                ("anthropic/claude-3-haiku-20240307", "Similar cost, different capabilities", 0.95)
            ]
        }

        return alternatives.get(current_model, [])

    def calculate_potential_savings(self,
                                  current_usage: Dict[str, Any],
                                  optimization_plan: Dict[str, str]) -> Dict[str, Any]:
        """Calculate potential cost savings from optimization plan"""

        current_cost = sum(stats["total_cost"] for stats in current_usage["model_usage"].values())
        projected_cost = 0

        # Simplified cost reduction factors
        cost_factors = {
            "openai/gpt-4o": 1.0,
            "openai/gpt-4o-mini": 0.1,  # 10x cheaper
            "anthropic/claude-3-haiku-20240307": 0.15,
            "local/llama-3.1-8b": 0.0  # Free
        }

        for current_model, target_model in optimization_plan.items():
            if current_model in current_usage["model_usage"]:
                current_model_cost = current_usage["model_usage"][current_model]["total_cost"]
                reduction_factor = cost_factors.get(target_model, 0.5)
                projected_cost += current_model_cost * reduction_factor

        savings = current_cost - projected_cost
        savings_percent = (savings / current_cost * 100) if current_cost > 0 else 0

        return {
            "current_cost": current_cost,
            "projected_cost": projected_cost,
            "potential_savings": savings,
            "savings_percentage": savings_percent
        }
```

## Performance Optimization

### Async Usage Tracking

```python
# High-performance async usage tracking
import asyncio
import dspy
from dspy.utils.usage_tracker import UsageTracker
from typing import Dict, Any, List
import json
import aiofiles
from datetime import datetime

class AsyncUsageTracker:
    """High-performance async usage tracking"""

    def __init__(self, batch_size: int = 100, flush_interval: int = 60):
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self.usage_buffer = []
        self.lock = asyncio.Lock()
        self.flushing_task = None

    async def start_background_flush(self):
        """Start background task to flush usage data"""

        async def flush_loop():
            while True:
                await asyncio.sleep(self.flush_interval)
                await self.flush_buffer()

        self.flushing_task = asyncio.create_task(flush_loop())

    async def track_async_operation(self, operation_data: Dict[str, Any]):
        """Track async operation usage"""

        async with self.lock:
            self.usage_buffer.append({
                **operation_data,
                "timestamp": datetime.now().isoformat()
            })

            if len(self.usage_buffer) >= self.batch_size:
                await self._flush_buffer_internal()

    async def flush_buffer(self):
        """Manually flush the usage buffer"""
        async with self.lock:
            await self._flush_buffer_internal()

    async def _flush_buffer_internal(self):
        """Internal buffer flushing logic"""

        if not self.usage_buffer:
            return

        # Write to file asynchronously
        filename = f"usage_batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        async with aiofiles.open(filename, 'w') as f:
            await f.write(json.dumps({
                "batch_size": len(self.usage_buffer),
                "records": self.usage_buffer
            }, indent=2))

        print(f"Flushed {len(self.usage_buffer)} usage records to {filename}")
        self.usage_buffer.clear()

    async def cleanup(self):
        """Clean up resources"""
        if self.flushing_task:
            self.flushing_task.cancel()
        await self.flush_buffer()

# Usage with async DSPy programs
async def async_usage_example():
    tracker = AsyncUsageTracker()
    await tracker.start_background_flush()

    # Configure DSPy
    lm = dspy.LM("openai/gpt-4o-mini")
    dspy.configure(lm=lm)
    program = dspy.asyncify(dspy.ChainOfThought("question -> answer"))

    # Simulate high-volume async operations
    tasks = []
    for i in range(50):
        async def process_question(q_id):
            with track_usage() as usage_tracker:
                result = await program(question=f"Question {q_id}")
                usage_data = usage_tracker.get_total_tokens()

                await tracker.track_async_operation({
                    "question_id": q_id,
                    "usage": usage_data,
                    "success": True
                })

        tasks.append(process_question(i))

    # Process all questions concurrently
    await asyncio.gather(*tasks)

    # Cleanup
    await tracker.cleanup()

# Run async example
# asyncio.run(async_usage_example())
```

## Speed Tips

- Use batch tracking for high-throughput applications to reduce I/O overhead
- Implement async usage tracking for concurrent DSPy operations
- Cache cost calculations to avoid repeated computations
- Use background threads for real-time monitoring to avoid blocking main execution
- Store usage data efficiently using time-series databases for large-scale deployments
- Implement usage data compression for long-term storage

## Common Pitfalls

- Not tracking usage in production leading to unexpected costs
- Missing token count details making cost attribution difficult
- Not implementing proper alerting for cost overruns
- Storing too much detailed usage data causing storage bloat
- Not aggregating usage data efficiently for analysis
- Missing async-compatible tracking in concurrent applications

## Best Practices Summary

- Always track usage in production environments with proper cost attribution
- Implement real-time cost monitoring with automated alerts
- Use batch processing for high-volume usage tracking
- Analyze usage patterns regularly to identify optimization opportunities
- Set up cost budgets and automatic limiting mechanisms
- Export usage data regularly for external analysis and reporting
- Implement proper data retention policies for usage analytics

## References

- [DSPy Usage Tracker](https://github.com/stanfordnlp/dspy/blob/main/dspy/utils/usage_tracker.py)
- [OpenAI Pricing](https://openai.com/pricing)
- [Anthropic Pricing](https://www.anthropic.com/pricing)
- [Cost Optimization Strategies](https://platform.openai.com/docs/guides/rate-limits)
- [Production Monitoring Best Practices](https://docs.python.org/3/library/logging.html)
