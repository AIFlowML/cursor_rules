---
description: DSPy 3.0.1 MultiChainComparison Module - Master ensemble reasoning through multiple chain comparison and synthesis
alwaysApply: false
---

> You are an expert in DSPy 3.0.1's MultiChainComparison module. Master ensemble reasoning by comparing multiple reasoning chains to produce superior, consensus-driven results.

## MultiChainComparison Architecture Flow

```
Generate Multiple → Extract Reasoning → Compare Chains → Synthesize Result → Return Best
      ↓                    ↓                ↓               ↓               ↓
Multiple Modules      Reasoning Steps    Chain Analysis   Consensus Build  Final Answer
Different Approaches  Pattern Extract    Quality Judge    Best Combine     Robust Output
      ↓                    ↓                ↓               ↓               ↓
Diverse Perspectives  Structured Input   Meta-Reasoning   Enhanced Quality Production Ready
```

## Instant Patterns

### Quick Start - Basic Chain Comparison

```python
import dspy

# Configure LM
lm = dspy.LM("openai/gpt-4o-mini", temperature=0.7)
dspy.configure(lm=lm)

# Create base reasoning module
base_reasoner = dspy.ChainOfThought("question -> reasoning, answer")

# Generate multiple completions
question = "Why do leaves change color in autumn?"
completions = []

for _ in range(3):
    result = base_reasoner(question=question)
    completions.append({
        "reasoning": result.reasoning,
        "answer": result.answer
    })

# Create comparison module
chain_comparator = dspy.MultiChainComparison(
    signature="question -> rationale, best_answer",
    M=3,  # Number of chains to compare
    temperature=0.3  # Lower temperature for final synthesis
)

# Compare and synthesize
final_result = chain_comparator(completions, question=question)
print(f"Synthesized reasoning: {final_result.rationale}")
print(f"Best answer: {final_result.best_answer}")
```

### Production Ready - Advanced Ensemble Reasoning

```python
import dspy
from typing import List, Dict, Any

class AdvancedAnalysisSignature(dspy.Signature):
    """Comprehensive analysis through ensemble reasoning comparison."""
    
    scenario: str = dspy.InputField(desc="Complex scenario to analyze")
    perspectives: List[str] = dspy.InputField(desc="Different analytical perspectives to consider")
    criteria: str = dspy.InputField(desc="Evaluation criteria for comparison")
    
    rationale: str = dspy.OutputField(
        desc="Synthesized reasoning combining the best insights from all approaches",
        prefix="Comprehensive Analysis:"
    )
    final_recommendation: str = dspy.OutputField(desc="Final recommendation based on ensemble analysis")
    confidence_score: float = dspy.OutputField(desc="Confidence in the final recommendation (0-1)")
    methodology_summary: str = dspy.OutputField(desc="Summary of the analytical methodology used")

class EnsembleReasoningSystem(dspy.Module):
    def __init__(self, num_chains=4):
        super().__init__()
        self.num_chains = num_chains
        
        # Different reasoning approaches for diversity
        self.analytical_reasoner = dspy.ChainOfThought(
            "scenario, perspective -> analytical_reasoning, analytical_conclusion"
        )
        
        self.creative_reasoner = dspy.ChainOfThought(
            "scenario, perspective -> creative_reasoning, creative_solution",
            rationale_field=dspy.OutputField(
                prefix="Creative exploration:",
                desc="Innovative and out-of-the-box reasoning"
            )
        )
        
        self.practical_reasoner = dspy.ChainOfThought(
            "scenario, perspective -> practical_reasoning, practical_recommendation"
        )
        
        self.critical_reasoner = dspy.ChainOfThought(
            "scenario, perspective -> critical_reasoning, critical_assessment"
        )
        
        # Chain comparison module
        self.chain_comparator = dspy.MultiChainComparison(
            signature=AdvancedAnalysisSignature,
            M=self.num_chains,
            temperature=0.2  # Conservative for synthesis
        )
        
        self.reasoners = [
            self.analytical_reasoner,
            self.creative_reasoner, 
            self.practical_reasoner,
            self.critical_reasoner
        ]
    
    def forward(self, scenario, perspectives, criteria):
        # Generate multiple reasoning chains
        completions = []
        
        for i, reasoner in enumerate(self.reasoners):
            # Use different perspectives for each reasoner
            perspective = perspectives[i % len(perspectives)]
            result = reasoner(scenario=scenario, perspective=perspective)
            
            # Extract reasoning and conclusion
            reasoning = getattr(result, 'analytical_reasoning', 
                              getattr(result, 'creative_reasoning',
                                     getattr(result, 'practical_reasoning',
                                            getattr(result, 'critical_reasoning', ''))))
            
            conclusion = getattr(result, 'analytical_conclusion',
                               getattr(result, 'creative_solution', 
                                      getattr(result, 'practical_recommendation',
                                             getattr(result, 'critical_assessment', ''))))
            
            completions.append({
                "reasoning": reasoning,
                "answer": conclusion,
                "approach": f"approach_{i+1}"
            })
        
        # Compare and synthesize chains
        synthesis = self.chain_comparator(
            completions,
            scenario=scenario,
            perspectives=perspectives,
            criteria=criteria
        )
        
        return dspy.Prediction(
            individual_chains=completions,
            **synthesis
        )

# Usage example
ensemble_system = EnsembleReasoningSystem(num_chains=4)

result = ensemble_system(
    scenario="Company considering transition to fully remote work model",
    perspectives=["Financial impact", "Employee wellbeing", "Productivity metrics", "Competitive advantage"],
    criteria="Balance cost savings with employee satisfaction and long-term business success"
)

print("Ensemble Analysis Results:")
print(f"Final Recommendation: {result.final_recommendation}")
print(f"Confidence: {result.confidence_score}")
print(f"Methodology: {result.methodology_summary}")
```

## Core MultiChainComparison Patterns

### Basic Chain Generation and Comparison

```python
# Template for generating diverse completions
class ChainGenerator:
    def __init__(self, base_signature):
        self.reasoners = [
            dspy.ChainOfThought(base_signature, temperature=0.3),   # Conservative
            dspy.ChainOfThought(base_signature, temperature=0.7),   # Balanced
            dspy.ChainOfThought(base_signature, temperature=0.9),   # Creative
        ]
    
    def generate_chains(self, **kwargs):
        completions = []
        for reasoner in self.reasoners:
            result = reasoner(**kwargs)
            completions.append({
                "reasoning": result.reasoning,
                "answer": getattr(result, 'answer', getattr(result, 'solution', ''))
            })
        return completions

# Usage with MultiChainComparison
generator = ChainGenerator("problem -> reasoning, solution")
comparator = dspy.MultiChainComparison("problem -> rationale, best_solution", M=3)

def solve_with_ensemble(problem):
    chains = generator.generate_chains(problem=problem)
    result = comparator(chains, problem=problem)
    return result

# Example usage
solution = solve_with_ensemble("How can we reduce energy consumption in office buildings?")
```

### Different Reasoning Perspectives

```python
class PerspectiveBasedReasoning(dspy.Module):
    def __init__(self, perspectives: List[str]):
        super().__init__()
        self.perspectives = perspectives
        
        # Create reasoners for each perspective
        self.perspective_reasoners = {}
        for perspective in perspectives:
            self.perspective_reasoners[perspective] = dspy.ChainOfThought(
                f"problem, {perspective}_focus -> {perspective}_reasoning, {perspective}_conclusion"
            )
        
        # Comparison module
        self.comparator = dspy.MultiChainComparison(
            "problem -> synthesized_reasoning, balanced_conclusion",
            M=len(perspectives),
            temperature=0.2
        )
    
    def forward(self, problem):
        completions = []
        
        for perspective in self.perspectives:
            reasoner = self.perspective_reasoners[perspective]
            # Create focused input for this perspective
            focused_input = {
                "problem": problem,
                f"{perspective}_focus": f"Analyze this problem from a {perspective} perspective"
            }
            
            result = reasoner(**focused_input)
            reasoning = getattr(result, f"{perspective}_reasoning", "")
            conclusion = getattr(result, f"{perspective}_conclusion", "")
            
            completions.append({
                "reasoning": f"[{perspective.upper()} PERSPECTIVE] {reasoning}",
                "answer": conclusion
            })
        
        # Synthesize perspectives
        synthesis = self.comparator(completions, problem=problem)
        
        return dspy.Prediction(
            perspectives_used=self.perspectives,
            individual_perspectives=completions,
            **synthesis
        )

# Usage with different analytical perspectives
business_analyzer = PerspectiveBasedReasoning([
    "financial", "operational", "strategic", "risk_management"
])

result = business_analyzer(
    problem="Should we expand into international markets next year?"
)
```

### Quality-Weighted Chain Comparison

```python
class QualityWeightedComparison(dspy.Module):
    def __init__(self, base_signature, num_chains=3):
        super().__init__()
        self.num_chains = num_chains
        
        # Generate chains with different approaches
        self.chain_generators = [
            dspy.ChainOfThought(base_signature),
            dspy.ProgramOfThought(base_signature),  # If computational
            dspy.ChainOfThought(base_signature, temperature=0.8)  # More creative
        ][:num_chains]
        
        # Quality evaluator
        self.quality_evaluator = dspy.Predict(
            "reasoning, answer, problem -> quality_score: float, quality_explanation: str"
        )
        
        # Weighted comparison
        self.comparator = dspy.MultiChainComparison(
            base_signature.append("quality_weights", dspy.InputField(desc="Quality weights for each chain")),
            M=num_chains
        )
    
    def forward(self, **kwargs):
        # Generate chains
        completions = []
        quality_scores = []
        
        for i, generator in enumerate(self.chain_generators):
            try:
                result = generator(**kwargs)
                reasoning = getattr(result, 'reasoning', '')
                answer = getattr(result, 'answer', getattr(result, 'solution', ''))
                
                # Evaluate quality
                quality_eval = self.quality_evaluator(
                    reasoning=reasoning,
                    answer=answer,
                    problem=str(kwargs)
                )
                
                completions.append({
                    "reasoning": reasoning,
                    "answer": answer,
                    "quality_score": quality_eval.quality_score,
                    "approach": f"method_{i+1}"
                })
                quality_scores.append(quality_eval.quality_score)
                
            except Exception as e:
                # Handle failed generations
                completions.append({
                    "reasoning": f"Generation failed: {str(e)}",
                    "answer": "Unable to generate answer",
                    "quality_score": 0.0,
                    "approach": f"method_{i+1}_failed"
                })
                quality_scores.append(0.0)
        
        # Weight chains by quality
        quality_weights = [f"Chain {i+1}: {score:.2f}" for i, score in enumerate(quality_scores)]
        
        # Compare with quality information
        synthesis = self.comparator(
            completions,
            quality_weights=str(quality_weights),
            **kwargs
        )
        
        return dspy.Prediction(
            individual_quality_scores=quality_scores,
            chain_details=completions,
            **synthesis
        )
```

## Advanced Chain Comparison Patterns

### Iterative Chain Refinement

```python
class IterativeChainRefinement(dspy.Module):
    def __init__(self, signature, max_iterations=3):
        super().__init__()
        self.max_iterations = max_iterations
        
        # Initial chain generator
        self.initial_generator = dspy.ChainOfThought(signature)
        
        # Chain refiner based on comparison feedback
        self.chain_refiner = dspy.ChainOfThought(
            signature.append("improvement_feedback", dspy.InputField(desc="Feedback for improvement"))
        )
        
        # Comparison module
        self.comparator = dspy.MultiChainComparison(
            signature.append("iteration", dspy.InputField(desc="Current iteration number")),
            M=3
        )
        
        # Quality assessor
        self.quality_assessor = dspy.Predict(
            "reasoning, answer -> improvement_needed: bool, feedback: str, quality_rating: float"
        )
    
    def forward(self, **kwargs):
        best_result = None
        best_quality = 0.0
        iteration_history = []
        
        for iteration in range(self.max_iterations):
            # Generate chains for this iteration
            completions = []
            
            for attempt in range(3):
                if iteration == 0:
                    # Initial generation
                    result = self.initial_generator(**kwargs)
                else:
                    # Refined generation based on feedback
                    improvement_feedback = iteration_history[-1].get('feedback', '')
                    result = self.chain_refiner(
                        improvement_feedback=improvement_feedback,
                        **kwargs
                    )
                
                reasoning = getattr(result, 'reasoning', '')
                answer = getattr(result, 'answer', getattr(result, 'solution', ''))
                
                completions.append({
                    "reasoning": reasoning,
                    "answer": answer
                })
            
            # Compare chains for this iteration
            comparison_result = self.comparator(
                completions,
                iteration=f"Iteration {iteration + 1}",
                **kwargs
            )
            
            # Assess quality and need for improvement
            quality_assessment = self.quality_assessor(
                reasoning=comparison_result.rationale,
                answer=getattr(comparison_result, 'answer', getattr(comparison_result, 'solution', ''))
            )
            
            iteration_data = {
                "iteration": iteration + 1,
                "result": comparison_result,
                "quality_rating": quality_assessment.quality_rating,
                "feedback": quality_assessment.feedback,
                "improvement_needed": quality_assessment.improvement_needed
            }
            
            iteration_history.append(iteration_data)
            
            # Track best result
            if quality_assessment.quality_rating > best_quality:
                best_quality = quality_assessment.quality_rating
                best_result = comparison_result
            
            # Stop if quality is sufficient
            if not quality_assessment.improvement_needed:
                break
        
        return dspy.Prediction(
            final_result=best_result,
            best_quality=best_quality,
            iteration_history=iteration_history,
            total_iterations=len(iteration_history)
        )
```

### Domain-Specific Chain Comparison

```python
class DomainExpertComparison(dspy.Module):
    def __init__(self, domain_experts: Dict[str, str]):
        super().__init__()
        self.experts = domain_experts
        
        # Create expert reasoners
        self.expert_reasoners = {}
        for expert_name, expertise in domain_experts.items():
            expert_signature = f"problem -> {expert_name}_analysis, {expert_name}_recommendation"
            self.expert_reasoners[expert_name] = dspy.ChainOfThought(
                expert_signature,
                instructions=f"You are a {expertise} expert. Analyze from your specialized perspective."
            )
        
        # Expert comparison module
        self.expert_comparator = dspy.MultiChainComparison(
            "problem, expert_domains -> expert_consensus, integrated_solution, confidence_level",
            M=len(domain_experts),
            temperature=0.1  # Conservative for expert synthesis
        )
    
    def forward(self, problem):
        expert_analyses = []
        
        for expert_name, reasoner in self.expert_reasoners.items():
            try:
                result = reasoner(problem=problem)
                analysis = getattr(result, f"{expert_name}_analysis", "")
                recommendation = getattr(result, f"{expert_name}_recommendation", "")
                
                expert_analyses.append({
                    "reasoning": f"[{expert_name.upper()} EXPERT] {analysis}",
                    "answer": recommendation,
                    "expert": expert_name
                })
            except Exception as e:
                expert_analyses.append({
                    "reasoning": f"[{expert_name.upper()} EXPERT] Analysis unavailable: {str(e)}",
                    "answer": "Unable to provide expert recommendation",
                    "expert": expert_name
                })
        
        # Synthesize expert opinions
        expert_domains = ", ".join([f"{name}: {expertise}" for name, expertise in self.experts.items()])
        
        synthesis = self.expert_comparator(
            expert_analyses,
            problem=problem,
            expert_domains=expert_domains
        )
        
        return dspy.Prediction(
            expert_opinions=expert_analyses,
            experts_consulted=list(self.experts.keys()),
            **synthesis
        )

# Usage example
medical_experts = DomainExpertComparison({
    "cardiologist": "heart and cardiovascular diseases",
    "neurologist": "nervous system and brain disorders", 
    "internist": "internal medicine and general adult care",
    "pharmacologist": "drug interactions and medication effects"
})

diagnosis_result = medical_experts(
    problem="Patient presents with chest pain, dizziness, and memory issues after starting new medication"
)
```

### Consensus Building with Conflict Resolution

```python
class ConsensusBuildingComparison(dspy.Module):
    def __init__(self, signature, disagreement_threshold=0.7):
        super().__init__()
        self.disagreement_threshold = disagreement_threshold
        
        # Multiple reasoning approaches
        self.reasoners = [
            dspy.ChainOfThought(signature, temperature=0.2),  # Conservative
            dspy.ChainOfThought(signature, temperature=0.5),  # Moderate  
            dspy.ChainOfThought(signature, temperature=0.8),  # Creative
        ]
        
        # Conflict detector
        self.conflict_detector = dspy.Predict(
            "reasoning_chains -> conflict_level: float, conflict_areas: str, resolution_needed: bool"
        )
        
        # Conflict resolver
        self.conflict_resolver = dspy.ChainOfThought(
            "problem, conflicting_reasoning, conflict_areas -> resolution_reasoning, consensus_solution"
        )
        
        # Final comparator
        self.comparator = dspy.MultiChainComparison(
            signature.append("conflict_resolution", dspy.InputField(desc="Conflict resolution context")),
            M=3
        )
    
    def forward(self, **kwargs):
        # Generate initial chains
        initial_chains = []
        for reasoner in self.reasoners:
            result = reasoner(**kwargs)
            reasoning = getattr(result, 'reasoning', '')
            answer = getattr(result, 'answer', getattr(result, 'solution', ''))
            
            initial_chains.append({
                "reasoning": reasoning,
                "answer": answer
            })
        
        # Detect conflicts
        chains_summary = "\n".join([f"Chain {i+1}: {chain['reasoning']}" for i, chain in enumerate(initial_chains)])
        conflict_analysis = self.conflict_detector(reasoning_chains=chains_summary)
        
        # Resolve conflicts if needed
        if conflict_analysis.resolution_needed and conflict_analysis.conflict_level > self.disagreement_threshold:
            # Generate resolution
            resolution = self.conflict_resolver(
                problem=str(kwargs),
                conflicting_reasoning=chains_summary,
                conflict_areas=conflict_analysis.conflict_areas
            )
            
            # Add resolution to chains
            conflict_resolution_context = f"Conflicts identified: {conflict_analysis.conflict_areas}. Resolution: {resolution.resolution_reasoning}"
            
            # Final comparison with conflict resolution
            final_result = self.comparator(
                initial_chains,
                conflict_resolution=conflict_resolution_context,
                **kwargs
            )
            
            return dspy.Prediction(
                conflict_detected=True,
                conflict_level=conflict_analysis.conflict_level,
                resolution_applied=resolution.consensus_solution,
                **final_result
            )
        else:
            # No significant conflicts, proceed with normal comparison
            final_result = self.comparator(
                initial_chains,
                conflict_resolution="No significant conflicts detected",
                **kwargs
            )
            
            return dspy.Prediction(
                conflict_detected=False,
                conflict_level=conflict_analysis.conflict_level,
                **final_result
            )
```

## Integration Patterns

### With Other DSPy Modules

```python
class HybridReasoningPipeline(dspy.Module):
    def __init__(self):
        super().__init__()
        
        # Stage 1: Multiple approaches to same problem
        self.logical_reasoner = dspy.ChainOfThought("problem -> logical_analysis, logical_conclusion")
        self.creative_reasoner = dspy.ChainOfThought("problem -> creative_analysis, creative_solution")  
        self.computational_reasoner = dspy.ProgramOfThought("problem -> computational_analysis, computed_result")
        
        # Stage 2: Compare approaches
        self.approach_comparator = dspy.MultiChainComparison(
            "problem -> approach_synthesis, recommended_solution",
            M=3
        )
        
        # Stage 3: Refine best solution
        self.solution_refiner = dspy.Refine(
            module=dspy.ChainOfThought("problem, initial_solution -> refined_reasoning, final_solution"),
            N=3,
            reward_fn=lambda inputs, pred: len(pred.final_solution.split()) / 50.0,  # Favor detailed solutions
            threshold=0.7
        )
    
    def forward(self, problem):
        # Generate multiple approaches
        approaches = []
        
        try:
            logical_result = self.logical_reasoner(problem=problem)
            approaches.append({
                "reasoning": logical_result.logical_analysis,
                "answer": logical_result.logical_conclusion
            })
        except Exception as e:
            approaches.append({
                "reasoning": f"Logical analysis failed: {str(e)}",
                "answer": "Logical approach unavailable"
            })
        
        try:
            creative_result = self.creative_reasoner(problem=problem)
            approaches.append({
                "reasoning": creative_result.creative_analysis,
                "answer": creative_result.creative_solution
            })
        except Exception as e:
            approaches.append({
                "reasoning": f"Creative analysis failed: {str(e)}",
                "answer": "Creative approach unavailable"
            })
        
        try:
            computational_result = self.computational_reasoner(problem=problem)
            approaches.append({
                "reasoning": computational_result.computational_analysis,
                "answer": computational_result.computed_result
            })
        except Exception as e:
            approaches.append({
                "reasoning": f"Computational analysis failed: {str(e)}",
                "answer": "Computational approach unavailable"
            })
        
        # Compare approaches
        comparison_result = self.approach_comparator(approaches, problem=problem)
        
        # Refine the recommended solution
        refined_result = self.solution_refiner(
            problem=problem,
            initial_solution=comparison_result.recommended_solution
        )
        
        return dspy.Prediction(
            approach_comparison=comparison_result.approach_synthesis,
            initial_recommendation=comparison_result.recommended_solution,
            final_solution=refined_result.final_solution,
            refinement_reasoning=refined_result.refined_reasoning,
            approaches_used=["logical", "creative", "computational"]
        )
```

## Speed Tips

### Efficient Chain Generation

```python
# Optimize chain generation for speed
class FastChainComparison:
    def __init__(self, signature):
        # Use focused, efficient reasoners
        self.fast_reasoners = [
            dspy.Predict(signature),  # Fastest - no reasoning overhead
            dspy.ChainOfThought(signature, temperature=0.1),  # Fast, focused reasoning
            dspy.ChainOfThought(signature, temperature=0.5)   # Balanced
        ]
        
        # Lightweight comparison
        self.quick_comparator = dspy.MultiChainComparison(
            signature,
            M=3,
            temperature=0.0  # Deterministic for speed
        )
    
    def compare(self, **kwargs):
        # Generate chains quickly
        completions = []
        for reasoner in self.fast_reasoners:
            result = reasoner(**kwargs)
            completions.append({
                "reasoning": getattr(result, 'reasoning', 'Direct prediction'),
                "answer": getattr(result, 'answer', getattr(result, 'solution', ''))
            })
        
        return self.quick_comparator(completions, **kwargs)

# Cached comparison for repeated similar problems
from functools import lru_cache

class CachedChainComparison(dspy.Module):
    def __init__(self, signature):
        super().__init__()
        self.comparator = dspy.MultiChainComparison(signature, M=3)
        self.chain_generator = ChainGenerator(signature)
    
    @lru_cache(maxsize=1000)
    def _cached_comparison(self, problem_hash):
        # This would be called with a hash of the problem
        # In practice, implement proper hashing for complex inputs
        chains = self.chain_generator.generate_chains(problem=problem_hash)
        return self.comparator(chains, problem=problem_hash)
    
    def forward(self, **kwargs):
        # Create a simple hash of the problem (in practice, use better hashing)
        problem_key = str(sorted(kwargs.items()))[:100]
        return self._cached_comparison(problem_key)
```

## Common Pitfalls

### Inadequate Chain Diversity

```python
# ❌ DON'T: Use identical or too-similar reasoning approaches
bad_chains = [
    dspy.ChainOfThought("problem -> answer"),
    dspy.ChainOfThought("problem -> answer"),  # Same approach!
    dspy.ChainOfThought("problem -> answer")   # No diversity!
]

# ✅ DO: Ensure diverse reasoning approaches
good_chains = [
    dspy.ChainOfThought("problem -> logical_reasoning, answer", temperature=0.2),
    dspy.ChainOfThought("problem -> creative_reasoning, answer", temperature=0.8), 
    dspy.ProgramOfThought("problem -> computational_reasoning, answer")  # Different module type
]
```

### Poor Chain Quality Control

```python
# ❌ DON'T: Ignore failed or low-quality chains
def bad_chain_handling(generators, problem):
    completions = []
    for generator in generators:
        result = generator(problem=problem)  # What if this fails?
        completions.append({
            "reasoning": result.reasoning,  # What if this attribute doesn't exist?
            "answer": result.answer
        })
    return completions

# ✅ DO: Handle failures and validate chain quality
def good_chain_handling(generators, problem):
    completions = []
    for i, generator in enumerate(generators):
        try:
            result = generator(problem=problem)
            reasoning = getattr(result, 'reasoning', f'Reasoning not available for generator {i}')
            answer = getattr(result, 'answer', getattr(result, 'solution', 'No answer generated'))
            
            # Validate quality
            if len(reasoning.strip()) > 10 and len(answer.strip()) > 5:
                completions.append({
                    "reasoning": reasoning,
                    "answer": answer
                })
            else:
                completions.append({
                    "reasoning": f"Low quality reasoning from generator {i}",
                    "answer": "Quality threshold not met"
                })
        except Exception as e:
            completions.append({
                "reasoning": f"Generator {i} failed: {str(e)}",
                "answer": "Generation failed"
            })
    
    return completions
```

### Incorrect Completion Format

```python
# ❌ DON'T: Provide incorrectly formatted completions
bad_completions = [
    {"reasoning": "Some reasoning"},  # Missing 'answer' key!
    {"answer": "Some answer"},       # Missing 'reasoning' key!
    {"rationale": "reasoning", "solution": "answer"}  # Wrong key names!
]

# ✅ DO: Ensure proper completion format
good_completions = [
    {"reasoning": "First reasoning approach", "answer": "First answer"},
    {"reasoning": "Second reasoning approach", "answer": "Second answer"},
    {"reasoning": "Third reasoning approach", "answer": "Third answer"}
]

# Validation helper
def validate_completions(completions, required_keys=["reasoning", "answer"]):
    validated = []
    for i, completion in enumerate(completions):
        validated_completion = {}
        for key in required_keys:
            # Handle different possible key names
            if key == "reasoning":
                value = completion.get("reasoning") or completion.get("rationale") or completion.get("analysis") or f"Reasoning {i+1}"
            elif key == "answer":
                value = completion.get("answer") or completion.get("solution") or completion.get("conclusion") or f"Answer {i+1}"
            else:
                value = completion.get(key, f"Default {key}")
            
            validated_completion[key] = value
        
        validated.append(validated_completion)
    
    return validated
```

## Best Practices Summary

- **Ensure diversity**: Use different reasoning approaches, temperatures, or modules for chain generation
- **Validate quality**: Check chain quality before comparison and handle failures gracefully
- **Proper formatting**: Ensure completions have correct "reasoning" and "answer" keys
- **Meaningful synthesis**: Design comparison signatures that produce actionable insights
- **Conflict resolution**: Implement mechanisms to handle disagreements between chains
- **Performance monitoring**: Track the quality and diversity of generated chains
- **Appropriate M value**: Match the number of chains (M) to your actual completions
- **Temperature control**: Use lower temperatures for final synthesis to ensure stability

## References

- [MultiChainComparison API Documentation](/docs/api/modules/MultiChainComparison.md)
- [Ensemble Reasoning Guide](/docs/guides/ensemble_reasoning.md)
- [Chain Comparison Tutorial](/docs/tutorials/chain_comparison/)
- [Advanced Consensus Building](/docs/examples/consensus_building/)