---
description: DSPy Tool Use Patterns - Safe tool integration and execution patterns for 10x development speed
alwaysApply: false
---

> You are an expert in DSPy 3.0.1 Tool Use Patterns for rapid agent development.

## Tool Integration Architecture

```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│ Tool Definition │───►│ Safety Wrapper   │───►│ Agent           │
│ (Type+Docstring)│    │ (Timeout+Error)  │    │ Integration     │
└─────────────────┘    └──────────────────┘    └─────────────────┘
                                │                       │
                                ▼                       ▼
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│ Sandbox         │◄───│ Execution        │◄───│ Tool Selection  │
│ Environment     │    │ Monitoring       │    │ & Validation    │
└─────────────────┘    └──────────────────┘    └─────────────────┘
         │                       │                       │
         ▼                       ▼                       ▼
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│ Result          │    │ Error Recovery   │    │ Performance     │
│ Processing      │    │ & Cleanup        │    │ Optimization    │
└─────────────────┘    └──────────────────┘    └─────────────────┘
```

## Instant Tool Patterns

### Quick Start Tool Integration

```python
import dspy
from typing import Any, Dict, List, Optional, Callable
import functools
import logging

# Minimal safe tool wrapper - copy-paste ready
def create_safe_tool(func: Callable, timeout: int = 30) -> Callable:
    """Create production-ready tool with safety wrappers"""

    @functools.wraps(func)
    def safe_wrapper(*args, **kwargs):
        import traceback
        import time

        start_time = time.time()

        try:
            # Input validation
            if len(str(args)[:500]) > 500:
                return {"error": f"Input too large for {func.__name__}", "status": "failed"}

            # Execute with basic monitoring
            result = func(*args, **kwargs)

            # Result validation
            execution_time = time.time() - start_time
            if execution_time > timeout:
                logging.warning(f"Tool {func.__name__} took {execution_time:.2f}s (timeout: {timeout}s)")

            return result

        except Exception as e:
            error_msg = f"Tool {func.__name__} failed: {str(e)[:200]}"
            logging.error(f"{error_msg}\n{traceback.format_exc()}")
            return {"error": error_msg, "tool": func.__name__, "status": "failed"}

    # Preserve metadata for DSPy
    safe_wrapper.__name__ = func.__name__
    safe_wrapper.__doc__ = func.__doc__ or f"Safe wrapper for {func.__name__}"

    return safe_wrapper

# Example tools with proper signatures
def search_web(query: str) -> str:
    """Search the web for information about the query"""
    # Simulate web search
    return f"Search results for '{query}': Found 5 relevant articles..."

def calculate_math(expression: str) -> float:
    """Calculate mathematical expressions safely"""
    # Safe evaluation
    try:
        # Basic safety - only allow certain operations
        safe_expr = expression.replace('^', '**')
        if any(dangerous in safe_expr for dangerous in ['import', '__', 'exec', 'eval']):
            raise ValueError("Dangerous operation detected")
        return eval(safe_expr)
    except:
        raise ValueError("Invalid mathematical expression")

def get_weather(city: str) -> dict:
    """Get current weather information for a city"""
    # Mock weather API
    return {
        "city": city,
        "temperature": "75°F",
        "condition": "Sunny",
        "humidity": "45%"
    }

# Create safe tools and agent
safe_tools = [
    create_safe_tool(search_web, timeout=20),
    create_safe_tool(calculate_math, timeout=5),
    create_safe_tool(get_weather, timeout=15)
]

# Create agent with safe tools
agent = dspy.ReAct("query -> response", tools=safe_tools, max_iters=8)
result = agent(query="Search for weather APIs and calculate 2^8")
```

### Production Tool Framework

```python
import dspy
from typing import Any, Dict, List, Optional, Callable, Union
import time
import asyncio
import logging
import json
import traceback
from dataclasses import dataclass
from enum import Enum

class ToolRiskLevel(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

@dataclass
class ToolMetadata:
    """Metadata for tool safety and monitoring"""
    name: str
    risk_level: ToolRiskLevel
    timeout: int
    max_retries: int
    requires_confirmation: bool = False
    allowed_environments: List[str] = None
    resource_limits: Dict[str, Any] = None

class ToolExecutionContext:
    """Context for tool execution with monitoring"""

    def __init__(self):
        self.start_time: float = 0
        self.end_time: float = 0
        self.attempt_count: int = 0
        self.errors: List[str] = []
        self.resource_usage: Dict[str, Any] = {}

    @property
    def execution_time(self) -> float:
        return self.end_time - self.start_time if self.end_time > 0 else time.time() - self.start_time

class ProductionToolFramework:
    """Production framework for safe tool execution"""

    def __init__(self, enable_monitoring: bool = True, enable_sandboxing: bool = True):
        self.tools: Dict[str, Callable] = {}
        self.metadata: Dict[str, ToolMetadata] = {}
        self.execution_stats: Dict[str, Dict[str, Any]] = {}
        self.enable_monitoring = enable_monitoring
        self.enable_sandboxing = enable_sandboxing

        # Initialize logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def register_tool(self,
                     func: Callable,
                     risk_level: ToolRiskLevel = ToolRiskLevel.LOW,
                     timeout: int = 30,
                     max_retries: int = 2,
                     requires_confirmation: bool = False,
                     **kwargs) -> str:
        """Register a tool with comprehensive safety metadata"""

        tool_name = func.__name__

        # Validate tool
        if not callable(func):
            raise ValueError(f"Tool {tool_name} is not callable")

        if not func.__doc__:
            self.logger.warning(f"Tool {tool_name} has no docstring")

        # Create metadata
        metadata = ToolMetadata(
            name=tool_name,
            risk_level=risk_level,
            timeout=timeout,
            max_retries=max_retries,
            requires_confirmation=requires_confirmation,
            **kwargs
        )

        # Create safe wrapper
        safe_tool = self._create_production_wrapper(func, metadata)

        # Register
        self.tools[tool_name] = safe_tool
        self.metadata[tool_name] = metadata
        self.execution_stats[tool_name] = {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "avg_execution_time": 0.0,
            "total_execution_time": 0.0,
            "error_types": {}
        }

        self.logger.info(f"Registered tool: {tool_name} (risk: {risk_level.value})")
        return tool_name

    def _create_production_wrapper(self, func: Callable, metadata: ToolMetadata) -> Callable:
        """Create comprehensive production wrapper"""

        @functools.wraps(func)
        def production_wrapper(*args, **kwargs):
            context = ToolExecutionContext()
            tool_name = metadata.name

            # Pre-execution validation
            if not self._validate_execution_environment(metadata):
                return {"error": f"Tool {tool_name} not allowed in current environment", "status": "denied"}

            if not self._validate_inputs(args, kwargs, metadata):
                return {"error": f"Invalid inputs for tool {tool_name}", "status": "invalid_input"}

            # Execute with retries
            for attempt in range(metadata.max_retries + 1):
                context.attempt_count = attempt + 1
                context.start_time = time.time()

                try:
                    # Sandboxed execution if enabled
                    if self.enable_sandboxing and metadata.risk_level in [ToolRiskLevel.MEDIUM, ToolRiskLevel.HIGH]:
                        result = self._sandboxed_execution(func, args, kwargs, metadata)
                    else:
                        result = self._direct_execution(func, args, kwargs, metadata)

                    context.end_time = time.time()

                    # Post-execution validation
                    validated_result = self._validate_result(result, metadata)

                    # Update success statistics
                    self._update_stats(tool_name, context, success=True)

                    return validated_result

                except Exception as e:
                    context.end_time = time.time()
                    error_msg = str(e)
                    context.errors.append(error_msg)

                    self.logger.error(f"Tool {tool_name} attempt {attempt + 1} failed: {error_msg}")

                    if attempt == metadata.max_retries:
                        # Final failure
                        self._update_stats(tool_name, context, success=False, error_type=type(e).__name__)

                        return {
                            "error": f"Tool {tool_name} failed after {metadata.max_retries + 1} attempts",
                            "last_error": error_msg[:200],
                            "attempts": attempt + 1,
                            "status": "failed"
                        }

                    # Brief delay before retry
                    time.sleep(min(2.0, 0.5 * (attempt + 1)))

        return production_wrapper

    def _validate_execution_environment(self, metadata: ToolMetadata) -> bool:
        """Validate if tool can execute in current environment"""
        if metadata.allowed_environments:
            # Could check environment variables, deployment stage, etc.
            current_env = "production"  # Placeholder
            return current_env in metadata.allowed_environments
        return True

    def _validate_inputs(self, args: tuple, kwargs: dict, metadata: ToolMetadata) -> bool:
        """Validate input parameters"""
        # Basic validation
        if len(args) > 10:  # Too many positional args
            return False

        if len(str(args)[:1000]) > 1000:  # Input too large
            return False

        # Risk-specific validation
        if metadata.risk_level == ToolRiskLevel.HIGH:
            # Additional validation for high-risk tools
            for arg in args:
                if isinstance(arg, str) and any(dangerous in arg.lower()
                                              for dangerous in ['rm -rf', 'DROP TABLE', 'DELETE FROM']):
                    return False

        return True

    def _direct_execution(self, func: Callable, args: tuple, kwargs: dict, metadata: ToolMetadata) -> Any:
        """Direct tool execution with timeout"""
        import signal

        def timeout_handler(signum, frame):
            raise TimeoutError(f"Tool {metadata.name} timed out after {metadata.timeout}s")

        if hasattr(signal, 'SIGALRM'):  # Unix systems
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(metadata.timeout)

        try:
            result = func(*args, **kwargs)
            if hasattr(signal, 'SIGALRM'):
                signal.alarm(0)  # Cancel timeout
            return result
        finally:
            if hasattr(signal, 'SIGALRM'):
                signal.alarm(0)

    def _sandboxed_execution(self, func: Callable, args: tuple, kwargs: dict, metadata: ToolMetadata) -> Any:
        """Sandboxed execution for high-risk tools"""

        # For demonstration - in practice would use actual sandboxing
        # like Docker containers, chroot jails, or process isolation
        self.logger.info(f"Executing {metadata.name} in sandbox mode")

        # Could implement:
        # - Resource limits (memory, CPU, file descriptors)
        # - Network restrictions
        # - File system access controls
        # - Process isolation

        return self._direct_execution(func, args, kwargs, metadata)

    def _validate_result(self, result: Any, metadata: ToolMetadata) -> Any:
        """Validate tool execution result"""

        # Size limits
        if isinstance(result, (str, list, dict)) and len(str(result)) > 10000:
            self.logger.warning(f"Tool {metadata.name} returned large result, truncating")
            return str(result)[:9900] + "... [truncated]"

        # Content validation for high-risk tools
        if metadata.risk_level in [ToolRiskLevel.HIGH, ToolRiskLevel.CRITICAL]:
            if isinstance(result, str) and any(sensitive in result.lower()
                                             for sensitive in ['password', 'secret', 'token']):
                self.logger.warning(f"Tool {metadata.name} may have returned sensitive data")

        return result

    def _update_stats(self, tool_name: str, context: ToolExecutionContext,
                     success: bool, error_type: str = None):
        """Update tool execution statistics"""

        stats = self.execution_stats[tool_name]

        stats["total_calls"] += 1
        if success:
            stats["successful_calls"] += 1
        else:
            stats["failed_calls"] += 1
            if error_type:
                stats["error_types"][error_type] = stats["error_types"].get(error_type, 0) + 1

        # Update timing
        execution_time = context.execution_time
        stats["total_execution_time"] += execution_time
        stats["avg_execution_time"] = stats["total_execution_time"] / stats["total_calls"]

        if self.enable_monitoring:
            self.logger.info(f"Tool {tool_name}: {'SUCCESS' if success else 'FAILED'} "
                           f"in {execution_time:.3f}s (attempt {context.attempt_count})")

    def get_tool_stats(self, tool_name: str = None) -> Dict[str, Any]:
        """Get execution statistics"""
        if tool_name:
            return self.execution_stats.get(tool_name, {})
        return self.execution_stats

    def get_registered_tools(self, risk_level: ToolRiskLevel = None) -> List[Callable]:
        """Get tools filtered by risk level"""
        if risk_level:
            return [self.tools[name] for name, meta in self.metadata.items()
                   if meta.risk_level == risk_level]
        return list(self.tools.values())

# Usage example
framework = ProductionToolFramework(enable_monitoring=True, enable_sandboxing=True)

# Register tools with different risk levels
framework.register_tool(
    search_web,
    risk_level=ToolRiskLevel.LOW,
    timeout=20,
    allowed_environments=["development", "production"]
)

framework.register_tool(
    calculate_math,
    risk_level=ToolRiskLevel.MEDIUM,
    timeout=5,
    max_retries=1
)

def execute_system_command(command: str) -> str:
    """Execute system command (HIGH RISK)"""
    import subprocess
    result = subprocess.run(command, shell=True, capture_output=True, text=True)
    return result.stdout

framework.register_tool(
    execute_system_command,
    risk_level=ToolRiskLevel.HIGH,
    timeout=10,
    max_retries=0,
    requires_confirmation=True,
    allowed_environments=["development"]  # Only in development
)

# Create agent with production tools
production_tools = framework.get_registered_tools()
agent = dspy.ReAct("task -> response", tools=production_tools, max_iters=10)

# Execute with full monitoring
result = agent(task="Search for Python best practices and calculate 10/2")
print("Execution stats:", framework.get_tool_stats())
```

## Core Tool Patterns

### 1. Python Code Execution (from PythonInterpreter analysis)

```python
import dspy
from dspy.primitives.python_interpreter import PythonInterpreter
import json
from typing import Any, Dict

class SafePythonExecutor:
    """Safe Python code execution based on DSPy PythonInterpreter patterns"""

    def __init__(self,
                 enable_file_access: bool = False,
                 allowed_read_paths: List[str] = None,
                 allowed_write_paths: List[str] = None):

        self.interpreter = None
        self.enable_file_access = enable_file_access
        self.allowed_read_paths = allowed_read_paths or []
        self.allowed_write_paths = allowed_write_paths or []

    def execute_code(self, code: str, variables: Dict[str, Any] = None) -> Any:
        """Execute Python code safely in sandbox"""

        # Initialize interpreter with safety constraints
        if not self.interpreter:
            self.interpreter = PythonInterpreter(
                enable_read_paths=self.allowed_read_paths if self.enable_file_access else None,
                enable_write_paths=self.allowed_write_paths if self.enable_file_access else None,
                sync_files=self.enable_file_access
            )

        try:
            with self.interpreter:
                result = self.interpreter.execute(code, variables or {})
                return result

        except Exception as e:
            return {"error": f"Code execution failed: {str(e)}", "code": code[:100]}

    def create_code_tool(self) -> Callable:
        """Create a tool function for agent use"""

        def execute_python_code(code: str, context_vars: str = "{}") -> Any:
            """Execute Python code in a safe sandbox environment

            Args:
                code: Python code to execute
                context_vars: JSON string of variables to inject (optional)
            """
            try:
                variables = json.loads(context_vars) if context_vars != "{}" else {}
            except:
                variables = {}

            return self.execute_code(code, variables)

        return execute_python_code

# Usage in agent
python_executor = SafePythonExecutor(enable_file_access=True)
code_tool = python_executor.create_code_tool()

# Tool for data analysis
def analyze_data(data_description: str) -> dict:
    """Analyze data using Python code generation"""

    analysis_code = f'''
import pandas as pd
import numpy as np

# Simulate data analysis for: {data_description}
data = np.random.randn(100, 3)
df = pd.DataFrame(data, columns=['A', 'B', 'C'])

analysis_result = {{
    'mean': df.mean().to_dict(),
    'std': df.std().to_dict(),
    'correlation': df.corr().iloc[0, 1]  # A-B correlation
}}

print("Analysis complete")
analysis_result
'''

    executor = SafePythonExecutor()
    return executor.execute_code(analysis_code)

# Create agent with code execution capabilities
tools = [code_tool, analyze_data]
code_agent = dspy.ReAct("task -> response", tools=tools, max_iters=8)

result = code_agent(task="Analyze the correlation between variables A and B in a dataset")
```

### 2. API Integration Tools

```python
import requests
from typing import Dict, Any, Optional
import time

class APIToolBuilder:
    """Builder for safe API integration tools"""

    def __init__(self, base_url: str, timeout: int = 30):
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        self.session = requests.Session()
        self.rate_limit_delay = 1.0
        self.last_request_time = 0

    def create_get_tool(self, endpoint: str, description: str = None) -> Callable:
        """Create a GET API tool"""

        def api_get_tool(params: str = "{}") -> Dict[str, Any]:
            """
            Make GET request to API endpoint

            Args:
                params: JSON string of query parameters
            """

            doc = description or f"GET request to {endpoint}"
            api_get_tool.__doc__ = doc

            try:
                # Rate limiting
                time_since_last = time.time() - self.last_request_time
                if time_since_last < self.rate_limit_delay:
                    time.sleep(self.rate_limit_delay - time_since_last)

                # Parse parameters
                query_params = json.loads(params) if params != "{}" else {}

                # Make request
                url = f"{self.base_url}/{endpoint.lstrip('/')}"
                response = self.session.get(url, params=query_params, timeout=self.timeout)

                self.last_request_time = time.time()

                # Handle response
                response.raise_for_status()

                result = {
                    "status_code": response.status_code,
                    "data": response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text,
                    "success": True
                }

                return result

            except requests.exceptions.RequestException as e:
                return {
                    "error": f"API request failed: {str(e)}",
                    "status_code": getattr(e.response, 'status_code', None),
                    "success": False
                }

        return api_get_tool

    def create_post_tool(self, endpoint: str, description: str = None) -> Callable:
        """Create a POST API tool"""

        def api_post_tool(data: str, headers: str = "{}") -> Dict[str, Any]:
            """
            Make POST request to API endpoint

            Args:
                data: JSON string of data to post
                headers: JSON string of additional headers
            """

            doc = description or f"POST request to {endpoint}"
            api_post_tool.__doc__ = doc

            try:
                # Rate limiting
                time_since_last = time.time() - self.last_request_time
                if time_since_last < self.rate_limit_delay:
                    time.sleep(self.rate_limit_delay - time_since_last)

                # Parse data and headers
                post_data = json.loads(data)
                post_headers = json.loads(headers) if headers != "{}" else {}

                # Default content type
                if 'Content-Type' not in post_headers:
                    post_headers['Content-Type'] = 'application/json'

                # Make request
                url = f"{self.base_url}/{endpoint.lstrip('/')}"
                response = self.session.post(url, json=post_data, headers=post_headers, timeout=self.timeout)

                self.last_request_time = time.time()

                # Handle response
                response.raise_for_status()

                return {
                    "status_code": response.status_code,
                    "data": response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text,
                    "success": True
                }

            except requests.exceptions.RequestException as e:
                return {
                    "error": f"API request failed: {str(e)}",
                    "status_code": getattr(e.response, 'status_code', None),
                    "success": False
                }

        return api_post_tool

# Usage example - create tools for a REST API
api_builder = APIToolBuilder("https://api.example.com", timeout=20)

# Create specific API tools
get_user = api_builder.create_get_tool("/users", "Get user information")
create_user = api_builder.create_post_tool("/users", "Create new user")
search_posts = api_builder.create_get_tool("/posts/search", "Search posts")

# Use in agent
api_tools = [get_user, create_user, search_posts]
api_agent = dspy.ReAct("task -> response", tools=api_tools, max_iters=6)

result = api_agent(task="Search for recent posts about AI and get user info")
```

### 3. File System Operations

```python
import os
import json
from pathlib import Path
from typing import List, Dict, Any, Union

class SafeFileSystem:
    """Safe file system operations for agents"""

    def __init__(self,
                 allowed_paths: List[str] = None,
                 max_file_size: int = 10*1024*1024,  # 10MB
                 allowed_extensions: List[str] = None):

        self.allowed_paths = [Path(p).resolve() for p in (allowed_paths or ["."])]
        self.max_file_size = max_file_size
        self.allowed_extensions = allowed_extensions or ['.txt', '.json', '.csv', '.md', '.py']

    def _validate_path(self, path: Union[str, Path]) -> Path:
        """Validate file path is within allowed directories"""

        path = Path(path).resolve()

        # Check if path is within allowed directories
        for allowed in self.allowed_paths:
            try:
                path.relative_to(allowed)
                return path
            except ValueError:
                continue

        raise ValueError(f"Path {path} not within allowed directories")

    def _validate_extension(self, path: Path):
        """Validate file extension"""
        if self.allowed_extensions and path.suffix not in self.allowed_extensions:
            raise ValueError(f"File extension {path.suffix} not allowed")

    def create_read_tool(self) -> Callable:
        """Create safe file reading tool"""

        def read_file(file_path: str, encoding: str = "utf-8") -> Dict[str, Any]:
            """Read content from a file safely

            Args:
                file_path: Path to file to read
                encoding: Text encoding (default: utf-8)
            """

            try:
                path = self._validate_path(file_path)
                self._validate_extension(path)

                if not path.exists():
                    return {"error": f"File {file_path} does not exist", "success": False}

                if path.stat().st_size > self.max_file_size:
                    return {"error": f"File {file_path} too large (max {self.max_file_size} bytes)", "success": False}

                with open(path, 'r', encoding=encoding) as f:
                    content = f.read()

                return {
                    "content": content,
                    "size": len(content),
                    "path": str(path),
                    "success": True
                }

            except Exception as e:
                return {"error": f"Failed to read file: {str(e)}", "success": False}

        return read_file

    def create_write_tool(self) -> Callable:
        """Create safe file writing tool"""

        def write_file(file_path: str, content: str, encoding: str = "utf-8", append: bool = False) -> Dict[str, Any]:
            """Write content to a file safely

            Args:
                file_path: Path to file to write
                content: Content to write
                encoding: Text encoding (default: utf-8)
                append: Whether to append or overwrite (default: overwrite)
            """

            try:
                path = self._validate_path(file_path)
                self._validate_extension(path)

                if len(content) > self.max_file_size:
                    return {"error": f"Content too large (max {self.max_file_size} bytes)", "success": False}

                # Create parent directories if needed
                path.parent.mkdir(parents=True, exist_ok=True)

                mode = 'a' if append else 'w'
                with open(path, mode, encoding=encoding) as f:
                    f.write(content)

                return {
                    "bytes_written": len(content.encode(encoding)),
                    "path": str(path),
                    "mode": "append" if append else "overwrite",
                    "success": True
                }

            except Exception as e:
                return {"error": f"Failed to write file: {str(e)}", "success": False}

        return write_file

    def create_list_tool(self) -> Callable:
        """Create safe directory listing tool"""

        def list_directory(dir_path: str, pattern: str = "*") -> Dict[str, Any]:
            """List contents of a directory safely

            Args:
                dir_path: Path to directory to list
                pattern: Glob pattern to filter files (default: *)
            """

            try:
                path = self._validate_path(dir_path)

                if not path.exists():
                    return {"error": f"Directory {dir_path} does not exist", "success": False}

                if not path.is_dir():
                    return {"error": f"Path {dir_path} is not a directory", "success": False}

                files = []
                directories = []

                for item in path.glob(pattern):
                    if item.is_file():
                        files.append({
                            "name": item.name,
                            "size": item.stat().st_size,
                            "extension": item.suffix,
                            "path": str(item.relative_to(path))
                        })
                    elif item.is_dir():
                        directories.append({
                            "name": item.name,
                            "path": str(item.relative_to(path))
                        })

                return {
                    "files": files,
                    "directories": directories,
                    "total_files": len(files),
                    "total_directories": len(directories),
                    "success": True
                }

            except Exception as e:
                return {"error": f"Failed to list directory: {str(e)}", "success": False}

        return list_directory

# Usage example
fs = SafeFileSystem(
    allowed_paths=["./workspace", "./data"],
    max_file_size=5*1024*1024,  # 5MB
    allowed_extensions=['.txt', '.json', '.csv', '.md']
)

# Create file system tools
read_file = fs.create_read_tool()
write_file = fs.create_write_tool()
list_dir = fs.create_list_tool()

# Create agent with file system capabilities
file_tools = [read_file, write_file, list_dir]
file_agent = dspy.ReAct("task -> response", tools=file_tools, max_iters=8)

result = file_agent(task="List files in workspace, read any JSON files, and create a summary")
```

## Performance Optimization

### Tool Result Caching

```python
import hashlib
import pickle
import time
from typing import Any, Callable, Dict, Optional

class ToolCache:
    """Intelligent caching for tool results"""

    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds

    def _generate_key(self, func_name: str, args: tuple, kwargs: dict) -> str:
        """Generate cache key from function call"""

        # Create hashable representation
        cache_data = {
            'function': func_name,
            'args': args,
            'kwargs': sorted(kwargs.items())
        }

        # Hash the representation
        serialized = pickle.dumps(cache_data)
        return hashlib.md5(serialized).hexdigest()

    def get(self, key: str) -> Optional[Any]:
        """Get cached result if valid"""

        if key not in self.cache:
            return None

        entry = self.cache[key]

        # Check TTL
        if time.time() - entry['timestamp'] > self.ttl_seconds:
            del self.cache[key]
            return None

        entry['access_count'] += 1
        entry['last_access'] = time.time()

        return entry['result']

    def set(self, key: str, result: Any):
        """Cache result with eviction policy"""

        # Eviction if cache is full
        if len(self.cache) >= self.max_size:
            # Remove least recently used
            lru_key = min(self.cache.keys(),
                         key=lambda k: self.cache[k]['last_access'])
            del self.cache[lru_key]

        self.cache[key] = {
            'result': result,
            'timestamp': time.time(),
            'last_access': time.time(),
            'access_count': 1
        }

    def create_cached_tool(self, func: Callable) -> Callable:
        """Create cached version of tool"""

        @functools.wraps(func)
        def cached_wrapper(*args, **kwargs):
            # Generate cache key
            cache_key = self._generate_key(func.__name__, args, kwargs)

            # Check cache first
            cached_result = self.get(cache_key)
            if cached_result is not None:
                logging.info(f"Cache HIT for {func.__name__}")
                return cached_result

            # Execute function
            logging.info(f"Cache MISS for {func.__name__}")
            result = func(*args, **kwargs)

            # Cache result if successful
            if not (isinstance(result, dict) and result.get('error')):
                self.set(cache_key, result)

            return result

        return cached_wrapper

# Usage
cache = ToolCache(max_size=500, ttl_seconds=1800)  # 30 minutes TTL

# Cache expensive operations
@cache.create_cached_tool
def expensive_computation(data: str) -> dict:
    """Expensive computation that benefits from caching"""
    time.sleep(2)  # Simulate expensive operation
    return {"result": f"Processed {len(data)} characters", "computation_time": 2.0}

cached_compute = cache.create_cached_tool(expensive_computation)
```

## Speed Tips

- **Type Hints**: Always include type hints for tool parameters - enables better agent reasoning
- **Docstrings**: Comprehensive docstrings help agents select correct tools
- **Error Isolation**: Wrap each tool individually to prevent cascade failures
- **Result Caching**: Cache expensive tool results with appropriate TTL
- **Async Execution**: Use async tools for I/O bound operations
- **Input Validation**: Validate inputs early to avoid expensive failures
- **Resource Limits**: Set memory/time limits to prevent resource exhaustion

## Common Pitfalls

1. **Missing Type Hints**: Agent can't understand tool parameters properly

   - **Solution**: Always include complete type annotations

2. **Poor Error Messages**: Unclear error responses confuse agents

   - **Solution**: Return structured error dictionaries with clear messages

3. **No Timeout Protection**: Tools hang indefinitely blocking agent execution

   - **Solution**: Implement timeout wrappers for all external calls

4. **Resource Leaks**: Long-running tools consume excessive resources

   - **Solution**: Use context managers and resource limits

5. **Security Vulnerabilities**: Direct system access without validation
   - **Solution**: Sandbox high-risk tools and validate all inputs

## Best Practices Summary

- **Safety First**: Sandbox and validate all tool inputs and outputs
- **Clear Interfaces**: Use descriptive names, type hints, and docstrings
- **Error Handling**: Return structured error responses for debugging
- **Resource Management**: Implement timeouts, limits, and cleanup
- **Monitoring**: Track tool usage and performance metrics
- **Caching**: Cache expensive operations with appropriate TTL
- **Testing**: Test tools in isolation before agent integration

## References

- **DSPy Tool Integration**: `dspy.adapters.types.tool.Tool` class
- **PythonInterpreter**: `/docs/dspy/dspy/primitives/python_interpreter.py`
- **Tool Use Tutorial**: `/docs/tutorials/tool_use/index.ipynb`
- **Agent Examples**: `/docs/tutorials/agents/index.ipynb`
- **Customer Service Tools**: `/docs/tutorials/customer_service_agent/index.ipynb`
