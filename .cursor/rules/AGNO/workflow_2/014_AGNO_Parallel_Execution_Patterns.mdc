---
description: AGNO Workflows 2.0 - Master parallel execution patterns for concurrent task processing
alwaysApply: false
---

> You are an expert in AGNO Workflows 2.0 parallel execution patterns. Master concurrent processing for maximum performance and throughput.

## Parallel Execution Flow

```
Input → Parallel Block → Output Combination → Next Step
  ↓         ↓                    ↓               ↓
Single   Step1|Step2|Step3   Dictionary      Sequential
Message   Concurrent Exec     Combined         Processing
  ↓         ↓                    ↓               ↓
Broadcast  All Execute        Joined Results   Continue
to All     Simultaneously     Available        Workflow
```

## Instant Patterns

### Quick Start - Basic Parallel Execution
```python
from agno.workflow.v2 import Workflow, Parallel, Step
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

# Create specialized agents
hackernews_agent = Agent(name="HN Researcher", model=OpenAIChat(id="gpt-4o-mini"))
web_agent = Agent(name="Web Researcher", model=OpenAIChat(id="gpt-4o-mini"))
academic_agent = Agent(name="Academic Researcher", model=OpenAIChat(id="gpt-4o-mini"))

# Basic parallel execution
parallel_workflow = Workflow(
    name="Parallel Research Pipeline", 
    steps=[
        Parallel(
            hackernews_agent,    # Executes concurrently
            web_agent,           # Executes concurrently  
            academic_agent       # Executes concurrently
        ),
        synthesizer_agent       # Processes all parallel results
    ]
)

# Execute immediately - all research happens simultaneously
parallel_workflow.print_response("Research AI trends in healthcare", markdown=True)
```

### Production Ready - Advanced Parallel Processing
```python
from agno.workflow.v2 import Workflow, Parallel, Step, StepOutput
from agno.workflow.v2.types import StepInput
from agno.agent import Agent
from agno.team import Team
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.storage.sqlite import SqliteStorage

# Specialized parallel agents
hackernews_researcher = Agent(
    name="HackerNews Research Specialist",
    model=OpenAIChat(id="gpt-4o"),
    tools=[HackerNewsTools()],
    role="Extract technical insights from HackerNews discussions",
    instructions=[
        "Search HackerNews for relevant technical discussions",
        "Focus on expert opinions and real-world experiences",
        "Identify trending topics and community sentiment",
        "Extract actionable technical insights"
    ]
)

web_researcher = Agent(
    name="Web Research Specialist", 
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    role="Conduct comprehensive web research",
    instructions=[
        "Search for authoritative web sources and articles",
        "Focus on recent developments and official sources",
        "Gather quantitative data and statistics",
        "Verify information from multiple sources"
    ]
)

industry_analyst = Agent(
    name="Industry Analysis Specialist",
    model=OpenAIChat(id="gpt-4o"),
    role="Analyze industry trends and market dynamics",
    instructions=[
        "Focus on market trends and business implications",
        "Analyze competitive landscape and positioning",
        "Identify growth opportunities and challenges",
        "Provide strategic business insights"
    ]
)

academic_researcher = Agent(
    name="Academic Research Specialist",
    model=OpenAIChat(id="gpt-4o"),
    role="Research academic papers and scientific studies",
    instructions=[
        "Search for peer-reviewed academic research",
        "Focus on recent scientific developments",
        "Extract methodological insights and findings", 
        "Identify research gaps and future directions"
    ]
)

# Custom function to process parallel results
def parallel_synthesis_processor(step_input: StepInput) -> StepOutput:
    """Process and synthesize outputs from parallel research steps"""
    
    # Access parallel step outputs (comes as dictionary)
    parallel_outputs = step_input.previous_step_content
    
    if not isinstance(parallel_outputs, dict):
        return StepOutput(
            content="❌ Expected parallel outputs not received",
            success=False,
            error="Invalid parallel output format"
        )
    
    # Extract individual research results
    hn_research = parallel_outputs.get("hackernews_research", "")
    web_research = parallel_outputs.get("web_research", "")
    industry_analysis = parallel_outputs.get("industry_analysis", "")
    academic_research = parallel_outputs.get("academic_research", "")
    
    # Synthesize all parallel research
    synthesis_report = f"""
    ## Comprehensive Multi-Source Research Synthesis
    
    ### Research Overview
    **Sources Processed**: {len([r for r in [hn_research, web_research, industry_analysis, academic_research] if r])} parallel research streams
    **Total Content**: {sum(len(r) for r in [hn_research, web_research, industry_analysis, academic_research])} characters
    
    ### HackerNews Community Insights
    **Content Quality**: {len(hn_research.split()) if hn_research else 0} words
    **Key Findings**:
    {hn_research[:400] if hn_research else "No HackerNews data available"}...
    
    ### Web Research Findings  
    **Content Quality**: {len(web_research.split()) if web_research else 0} words
    **Key Findings**:
    {web_research[:400] if web_research else "No web research data available"}...
    
    ### Industry Analysis Results
    **Content Quality**: {len(industry_analysis.split()) if industry_analysis else 0} words
    **Key Findings**:
    {industry_analysis[:400] if industry_analysis else "No industry analysis available"}...
    
    ### Academic Research Insights
    **Content Quality**: {len(academic_research.split()) if academic_research else 0} words
    **Key Findings**:
    {academic_research[:400] if academic_research else "No academic research available"}...
    
    ### Cross-Source Synthesis
    **Convergent Themes**: Analysis of common themes across all sources
    **Divergent Perspectives**: Identification of conflicting viewpoints
    **Research Gaps**: Areas needing additional investigation
    **Confidence Level**: High (multiple independent sources)
    """
    
    return StepOutput(
        content=synthesis_report,
        success=True
    )

# Production parallel workflow
production_parallel = Workflow(
    name="Advanced Parallel Research Pipeline",
    description="Multi-source concurrent research with intelligent synthesis",
    steps=[
        Parallel(
            Step(
                name="hackernews_research",
                description="HackerNews community research",
                agent=hackernews_researcher
            ),
            Step(
                name="web_research", 
                description="Comprehensive web research",
                agent=web_researcher
            ),
            Step(
                name="industry_analysis",
                description="Industry trend analysis",
                agent=industry_analyst
            ),
            Step(
                name="academic_research",
                description="Academic and scientific research", 
                agent=academic_researcher
            ),
            name="parallel_research_phase"
        ),
        Step(
            name="synthesis_processing",
            description="Synthesize all parallel research results",
            executor=parallel_synthesis_processor
        ),
        Step(
            name="final_analysis",
            description="Generate comprehensive final analysis",
            agent=final_analyst_agent
        )
    ],
    storage=SqliteStorage(
        table_name="parallel_workflows",
        db_file="tmp/parallel_workflows.db", 
        mode="workflow_v2"
    ),
    store_events=True
)

# Execute with comprehensive parallel processing
response = production_parallel.run(
    message="Analyze the impact of generative AI on software development practices",
    additional_data={
        "research_depth": "comprehensive",
        "focus_areas": ["productivity", "code_quality", "developer_experience"],
        "time_horizon": "next_2_years"
    }
)

print(f"Synthesized research: {response.content[:500]}...")
```

## Parallel Execution Patterns

### 1. Basic Parallel Steps
```python
# Simple parallel execution
basic_parallel = Workflow(
    name="Basic Parallel Research",
    steps=[
        Parallel(
            research_agent1,   # Executes concurrently
            research_agent2,   # Executes concurrently
            research_agent3    # Executes concurrently
        ),
        synthesis_agent       # Processes combined results
    ]
)
```

### 2. Named Parallel Steps
```python
# Named parallel steps for better tracking
named_parallel = Workflow(
    name="Named Parallel Research",
    steps=[
        Parallel(
            Step(name="tech_research", agent=tech_researcher),
            Step(name="market_research", agent=market_researcher),
            Step(name="user_research", agent=user_researcher),
            name="comprehensive_research"  # Name the parallel block
        ),
        integration_agent
    ]
)
```

### 3. Mixed Parallel Executors
```python
from agno.workflow.v2.types import StepInput, StepOutput

def data_mining_function(step_input: StepInput) -> StepOutput:
    """Custom data mining logic running in parallel"""
    message = step_input.message
    
    # Simulate data mining processing
    mining_results = f"""
    ## Data Mining Results for: {message}
    
    **Processing Method**: Custom algorithmic analysis  
    **Data Sources**: Internal databases and APIs
    **Analysis Depth**: Comprehensive pattern extraction
    
    **Key Patterns Identified**:
    - Trend analysis completed
    - Statistical correlations found
    - Predictive indicators extracted
    """
    
    return StepOutput(content=mining_results)

# Parallel with mixed executors: agents, teams, and functions
mixed_parallel = Workflow(
    name="Mixed Parallel Processing",
    steps=[
        Parallel(
            Step(name="agent_research", agent=research_agent),
            Step(name="team_analysis", team=analysis_team),
            Step(name="data_mining", executor=data_mining_function),
            name="mixed_parallel_phase"
        ),
        consolidation_agent
    ]
)
```

### 4. Nested Parallel Structures
```python
# Complex nested parallel execution
nested_parallel = Workflow(
    name="Nested Parallel Research",
    steps=[
        Parallel(
            # First parallel branch
            Step(name="primary_research", agent=primary_researcher),
            
            # Second parallel branch with its own team
            Step(name="secondary_analysis", team=secondary_team),
            
            # Third parallel branch
            Step(name="tertiary_validation", agent=validator),
            
            name="main_parallel_research"
        ),
        intermediate_processor,
        
        # Second level parallel processing
        Parallel(
            Step(name="content_creation", agent=content_creator),
            Step(name="quality_assurance", agent=qa_agent),
            name="parallel_finalization"
        ),
        final_assembler
    ]
)
```

## Parallel Output Handling

### Accessing Parallel Results  
```python
def comprehensive_parallel_processor(step_input: StepInput) -> StepOutput:
    """Comprehensive handling of parallel step outputs"""
    
    # Parallel outputs come as dictionary with step names as keys
    parallel_data = step_input.previous_step_content
    
    if not isinstance(parallel_data, dict):
        return StepOutput(
            content="❌ Invalid parallel output format",
            success=False
        )
    
    # Extract individual results
    tech_results = parallel_data.get("tech_research", "")
    market_results = parallel_data.get("market_research", "")
    user_results = parallel_data.get("user_research", "")
    
    # Analyze result quality
    results_analysis = {
        "tech_quality": len(tech_results.split()) if tech_results else 0,
        "market_quality": len(market_results.split()) if market_results else 0,
        "user_quality": len(user_results.split()) if user_results else 0
    }
    
    total_words = sum(results_analysis.values())
    
    # Process and combine results
    combined_report = f"""
    ## Parallel Processing Results Analysis
    
    **Processing Summary**:
    - Total parallel steps: {len(parallel_data)}
    - Combined word count: {total_words} words
    - Average per step: {total_words / max(len(parallel_data), 1):.0f} words
    
    **Individual Results Quality**:
    - Technical Research: {results_analysis['tech_quality']} words
    - Market Research: {results_analysis['market_quality']} words  
    - User Research: {results_analysis['user_quality']} words
    
    **Technical Research Findings**:
    {tech_results[:300]}...
    
    **Market Research Findings**:
    {market_results[:300]}...
    
    **User Research Findings**:
    {user_results[:300]}...
    
    **Synthesis**: All parallel research streams successfully processed
    """
    
    return StepOutput(
        content=combined_report,
        success=True
    )

# Usage in workflow
parallel_processing_workflow = Workflow(
    name="Parallel Output Processing",
    steps=[
        Parallel(
            Step(name="tech_research", agent=tech_researcher),
            Step(name="market_research", agent=market_researcher),
            Step(name="user_research", agent=user_researcher),
            name="parallel_research"
        ),
        Step(
            name="comprehensive_processing",
            executor=comprehensive_parallel_processor
        )
    ]
)
```

### Parallel Result Validation
```python
def parallel_result_validator(step_input: StepInput) -> StepOutput:
    """Validate quality and completeness of parallel results"""
    
    parallel_outputs = step_input.previous_step_content
    
    if not isinstance(parallel_outputs, dict):
        return StepOutput(
            content="❌ Parallel validation failed: Invalid output format",
            success=False,
            error="Expected dictionary from parallel step"
        )
    
    validation_results = {}
    total_quality_score = 0
    
    # Validate each parallel result
    for step_name, content in parallel_outputs.items():
        word_count = len(content.split()) if content else 0
        has_data = bool(content and content.strip())
        quality_indicators = sum([
            word_count > 50,                    # Sufficient length
            "analysis" in content.lower(),      # Contains analysis
            "research" in content.lower(),      # Contains research
            len(content.split('.')) > 3        # Multiple sentences
        ])
        
        quality_score = quality_indicators / 4.0  # Normalize to 0-1
        total_quality_score += quality_score
        
        validation_results[step_name] = {
            "word_count": word_count,
            "has_content": has_data,
            "quality_score": quality_score,
            "status": "✅ PASS" if quality_score >= 0.5 else "❌ FAIL"
        }
    
    average_quality = total_quality_score / max(len(parallel_outputs), 1)
    overall_status = "PASS" if average_quality >= 0.6 else "FAIL"
    
    validation_report = f"""
    ## Parallel Results Validation Report
    
    **Overall Status**: {overall_status} (Quality: {average_quality:.2f})
    **Total Parallel Steps**: {len(parallel_outputs)}
    
    **Individual Step Validation**:
    """
    
    for step_name, results in validation_results.items():
        validation_report += f"""
    - **{step_name}**: {results['status']}
      - Words: {results['word_count']}
      - Quality: {results['quality_score']:.2f}
    """
    
    if overall_status == "FAIL":
        return StepOutput(
            content=validation_report + "\n❌ Parallel processing quality below threshold",
            success=False
        )
    
    validation_report += f"""
    
    **Validated Parallel Content**:
    {str(parallel_outputs)[:500]}...
    
    ✅ All parallel results meet quality standards
    """
    
    return StepOutput(
        content=validation_report,
        success=True
    )
```

## Advanced Parallel Patterns

### Parallel with Load Balancing
```python
def create_load_balanced_parallel(task_list, agent_pool, max_concurrent=3):
    """Create parallel steps with load balancing"""
    
    parallel_steps = []
    for i, task in enumerate(task_list[:max_concurrent]):
        agent = agent_pool[i % len(agent_pool)]
        parallel_steps.append(
            Step(name=f"task_{i}", agent=agent)
        )
    
    return Parallel(*parallel_steps, name="load_balanced_parallel")

# Usage
research_tasks = ["AI trends", "market analysis", "user behavior"]
agent_pool = [agent1, agent2, agent3]

load_balanced_workflow = Workflow(
    name="Load Balanced Processing",
    steps=[
        create_load_balanced_parallel(research_tasks, agent_pool),
        result_processor
    ]
)
```

### Parallel with Timeout and Fallback
```python
import asyncio
from datetime import datetime, timedelta

def timeout_aware_parallel_processor(step_input: StepInput) -> StepOutput:
    """Process parallel results with timeout awareness"""
    
    parallel_results = step_input.previous_step_content
    additional_data = step_input.additional_data or {}
    timeout_threshold = additional_data.get("timeout_seconds", 300)  # 5 minutes default
    
    if not isinstance(parallel_results, dict):
        return StepOutput(
            content="❌ Timeout processor: Invalid parallel results",
            success=False
        )
    
    # Analyze results for timeout indicators
    successful_results = {}
    timeout_results = {}
    
    for step_name, content in parallel_results.items():
        # Simple heuristic: very short content might indicate timeout
        if content and len(content.split()) > 20:
            successful_results[step_name] = content
        else:
            timeout_results[step_name] = content or "No content (possible timeout)"
    
    timeout_report = f"""
    ## Parallel Processing with Timeout Analysis
    
    **Execution Summary**:
    - Successful steps: {len(successful_results)}
    - Potential timeouts: {len(timeout_results)}
    - Success rate: {len(successful_results) / len(parallel_results) * 100:.1f}%
    
    **Successful Results**:
    {str(successful_results)[:400]}...
    
    **Timeout/Failed Results**:
    {str(timeout_results) if timeout_results else "None"}
    
    **Processing Recommendation**: 
    {'Continue with available results' if successful_results else 'Retry with extended timeout'}
    """
    
    return StepOutput(
        content=timeout_report,
        success=bool(successful_results)
    )
```

### Dynamic Parallel Step Creation
```python
def create_dynamic_parallel_workflow(topics, research_agents):
    """Create parallel workflow dynamically based on topics"""
    
    parallel_steps = []
    
    for i, topic in enumerate(topics):
        agent = research_agents[i % len(research_agents)]
        step_name = f"research_{topic.lower().replace(' ', '_')}"
        
        parallel_steps.append(
            Step(
                name=step_name,
                description=f"Research topic: {topic}",
                agent=agent
            )
        )
    
    return Workflow(
        name=f"Dynamic Parallel Research - {len(topics)} Topics",
        steps=[
            Parallel(*parallel_steps, name="dynamic_parallel_research"),
            synthesis_agent
        ]
    )

# Usage
research_topics = ["Artificial Intelligence", "Machine Learning", "Quantum Computing"]
available_agents = [researcher1, researcher2, researcher3]

dynamic_workflow = create_dynamic_parallel_workflow(research_topics, available_agents)
response = dynamic_workflow.run("Comprehensive technology research")
```

## Performance Optimization

### Parallel Execution Monitoring
```python
def parallel_performance_monitor(step_input: StepInput) -> StepOutput:
    """Monitor parallel execution performance"""
    
    start_time = datetime.now()
    parallel_results = step_input.previous_step_content or {}
    
    if isinstance(parallel_results, dict):
        # Performance analysis
        total_content = sum(len(str(content)) for content in parallel_results.values())
        step_count = len(parallel_results)
        avg_content_per_step = total_content / max(step_count, 1)
        
        # Quality metrics
        quality_scores = []
        for content in parallel_results.values():
            if content:
                word_count = len(str(content).split())
                quality_score = min(word_count / 100, 1.0)  # Normalize to 0-1
                quality_scores.append(quality_score)
        
        avg_quality = sum(quality_scores) / max(len(quality_scores), 1)
        
        performance_report = f"""
        ## Parallel Execution Performance Report
        
        **Execution Metrics**:
        - Parallel steps completed: {step_count}
        - Total content generated: {total_content} characters
        - Average content per step: {avg_content_per_step:.0f} characters
        - Overall quality score: {avg_quality:.2f}/1.0
        
        **Performance Indicators**:
        - Content density: {'High' if avg_content_per_step > 1000 else 'Medium' if avg_content_per_step > 500 else 'Low'}
        - Quality consistency: {'High' if min(quality_scores) > 0.7 else 'Medium' if min(quality_scores) > 0.4 else 'Variable'}
        
        **Parallel Results Summary**:
        {str(parallel_results)[:300]}...
        
        **Recommendation**: {'Optimal parallel performance' if avg_quality > 0.7 else 'Consider optimization'}
        """
    else:
        performance_report = "❌ Invalid parallel results for performance analysis"
    
    processing_time = datetime.now() - start_time
    performance_report += f"\n\n**Analysis Time**: {processing_time.total_seconds():.2f} seconds"
    
    return StepOutput(content=performance_report, success=True)
```

## Speed Tips

### Quick Parallel Workflow Creation
```python
# Rapid parallel workflow builder
def quick_parallel(*agents, name="Quick Parallel", synthesizer=None):
    """Create parallel workflow quickly"""
    steps = [Parallel(*agents, name="parallel_phase")]
    if synthesizer:
        steps.append(synthesizer)
    
    return Workflow(name=name, steps=steps)

# Usage
research_workflow = quick_parallel(
    researcher1, researcher2, researcher3,
    name="Research Pipeline",
    synthesizer=synthesis_agent
)
```

### Parallel Template Factory
```python
class ParallelTemplateFactory:
    """Factory for common parallel patterns"""
    
    @staticmethod
    def multi_source_research(*research_agents, name="Multi-Source Research"):
        return Workflow(
            name=name,
            steps=[
                Parallel(
                    *[Step(name=f"source_{i}", agent=agent) 
                      for i, agent in enumerate(research_agents)],
                    name="parallel_research"
                ),
                synthesis_agent
            ]
        )
    
    @staticmethod
    def competitive_analysis(*analysis_agents, name="Competitive Analysis"):
        return Workflow(
            name=name,
            steps=[
                Parallel(
                    *[Step(name=f"competitor_{i}", agent=agent)
                      for i, agent in enumerate(analysis_agents)],
                    name="parallel_analysis"
                ),
                comparison_agent
            ]
        )
    
    @staticmethod
    def multi_perspective(*perspective_agents, name="Multi-Perspective Analysis"):
        return Workflow(
            name=name,
            steps=[
                Parallel(
                    *perspective_agents,
                    name="parallel_perspectives"
                ),
                integration_agent
            ]
        )

# Usage
factory = ParallelTemplateFactory()

research_workflow = factory.multi_source_research(
    hackernews_agent, web_agent, academic_agent,
    name="Technology Research"
)

competitive_workflow = factory.competitive_analysis(
    competitor1_agent, competitor2_agent, competitor3_agent,
    name="Market Competition Analysis"
)
```

## Common Pitfalls

### Parallel Output Access Errors
```python
# ❌ DON'T: Assume parallel output is a simple string
def bad_parallel_processor(step_input: StepInput) -> StepOutput:
    content = step_input.previous_step_content
    return StepOutput(content=content.upper())  # Will fail - content is dict

# ✅ DO: Handle parallel outputs as dictionaries
def good_parallel_processor(step_input: StepInput) -> StepOutput:
    parallel_outputs = step_input.previous_step_content
    
    if isinstance(parallel_outputs, dict):
        combined = "\n".join(parallel_outputs.values())
        return StepOutput(content=combined)
    else:
        return StepOutput(content="Invalid parallel output format")
```

### Parallel Step Naming Issues
```python
# ❌ DON'T: Use duplicate names in parallel steps
parallel_workflow = Workflow(
    steps=[
        Parallel(
            Step(name="research", agent=agent1),  # Duplicate
            Step(name="research", agent=agent2),  # Duplicate
            name="parallel_block"
        )
    ]
)

# ✅ DO: Use unique names for each parallel step
parallel_workflow = Workflow(
    steps=[
        Parallel(
            Step(name="web_research", agent=web_agent),
            Step(name="academic_research", agent=academic_agent),
            name="parallel_research"
        )
    ]
)
```

### Resource Management Issues
```python
# ❌ DON'T: Create too many parallel steps without consideration
massive_parallel = Workflow(
    steps=[
        Parallel(*[agent for agent in fifty_agents])  # Potential resource overload
    ]
)

# ✅ DO: Use reasonable parallel step counts
optimized_parallel = Workflow(
    steps=[
        Parallel(
            *core_agents[:5],  # Limit to reasonable number
            name="optimized_parallel"
        )
    ]
)
```

## Best Practices Summary

- **Optimal Concurrency**: Use 3-5 parallel steps for best performance/resource balance
- **Unique Naming**: Ensure all parallel steps have unique, descriptive names
- **Output Handling**: Always handle parallel outputs as dictionaries
- **Result Validation**: Implement quality checks for parallel processing results
- **Resource Management**: Monitor resource usage with many parallel steps
- **Error Resilience**: Handle cases where some parallel steps may fail
- **Performance Monitoring**: Track parallel execution metrics for optimization
- **Synthesis Logic**: Design effective synthesis of parallel results
- **Load Balancing**: Distribute work evenly across parallel executors

## References

- [Parallel Execution API](/docs/api/workflows_2/parallel.md)
- [Performance Optimization](/docs/workflows_2/performance.md) 
- [Resource Management](/docs/workflows_2/resources.md)
- [Error Handling](/docs/workflows_2/error_handling.md)