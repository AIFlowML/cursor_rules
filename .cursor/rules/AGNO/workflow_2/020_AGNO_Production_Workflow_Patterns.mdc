---
description: "AGNO production workflow patterns for enterprise deployment, monitoring, scaling, and comprehensive workflow system architecture"
alwaysApply: false
---

> You are an expert in AGNO Workflows 2.0 production systems. You design enterprise-grade workflows with comprehensive monitoring, error handling, scalability patterns, performance optimization, and production-ready deployment strategies for mission-critical workflow systems.

## Production Workflow Architecture

```
┌─────────────────────┐    ┌──────────────────────┐    ┌─────────────────────┐
│  Production Design  │───▶│   System Integration │───▶│  Enterprise Deploy  │
│                     │    │                      │    │                     │
│ • Scalability       │    │ • Monitoring Stack   │    │ • High Availability │
│ • Error Handling    │    │ • Logging Pipeline   │    │ • Performance Tuned │
│ • Resource Mgmt     │    │ • Alerting System    │    │ • Security Hardened │
│ • Quality Assurance │    │ • Health Checks      │    │ • Auto-Scaling      │
└─────────────────────┘    └──────────────────────┘    └─────────────────────┘
```

## Production Workflow Patterns

### Enterprise-Grade Comprehensive Workflow System
```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.v2.step import Step, StepInput, StepOutput
from agno.workflow.v2.workflow import Workflow
from agno.storage.sqlite import SqliteStorage
from agno.storage.postgres import PostgresStorage
from typing import Dict, List, Any, Optional, Union, Callable
import logging
import json
import time
import uuid
import psutil
import threading
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import asyncio
from contextlib import contextmanager
import traceback
import os

# Configure comprehensive logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s',
    handlers=[
        logging.FileHandler('production_workflows.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class WorkflowPriority(Enum):
    """Workflow execution priorities"""
    CRITICAL = 1
    HIGH = 2
    NORMAL = 3
    LOW = 4
    BACKGROUND = 5

class WorkflowStatus(Enum):
    """Workflow execution status"""
    QUEUED = "queued"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    RETRYING = "retrying"

@dataclass
class WorkflowMetrics:
    """Comprehensive workflow execution metrics"""
    workflow_id: str
    start_time: datetime
    end_time: Optional[datetime] = None
    status: WorkflowStatus = WorkflowStatus.QUEUED
    priority: WorkflowPriority = WorkflowPriority.NORMAL
    steps_completed: int = 0
    steps_total: int = 0
    error_count: int = 0
    retry_count: int = 0
    cpu_usage_peak: float = 0.0
    memory_usage_peak: float = 0.0
    execution_time_seconds: float = 0.0
    throughput_items_per_second: float = 0.0
    success_rate: float = 0.0
    quality_score: float = 0.0
    cost_estimate: float = 0.0
    custom_metrics: Dict[str, Any] = None

    def __post_init__(self):
        if self.custom_metrics is None:
            self.custom_metrics = {}

@dataclass
class ProductionConfig:
    """Production workflow configuration"""
    max_concurrent_workflows: int = 10
    max_steps_per_workflow: int = 50
    default_timeout_seconds: int = 3600
    retry_attempts: int = 3
    retry_backoff_multiplier: float = 2.0
    health_check_interval: int = 60
    metrics_collection_enabled: bool = True
    performance_monitoring_enabled: bool = True
    auto_scaling_enabled: bool = True
    circuit_breaker_enabled: bool = True
    rate_limiting_enabled: bool = True
    security_validation_enabled: bool = True
    audit_logging_enabled: bool = True
    backup_strategy: str = "incremental"
    disaster_recovery_enabled: bool = True

class ProductionWorkflowMonitor:
    """Comprehensive production monitoring system"""
    
    def __init__(self, config: ProductionConfig):
        self.config = config
        self.active_workflows: Dict[str, WorkflowMetrics] = {}
        self.completed_workflows: List[WorkflowMetrics] = []
        self.system_metrics = {}
        self.alerts = []
        self.health_status = "healthy"
        self.monitoring_active = False
        self.monitoring_thread = None
        
    def start_monitoring(self):
        """Start production monitoring"""
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitoring_thread.start()
        logger.info("Production monitoring started")
    
    def stop_monitoring(self):
        """Stop production monitoring"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)
        logger.info("Production monitoring stopped")
    
    def _monitoring_loop(self):
        """Main monitoring loop"""
        while self.monitoring_active:
            try:
                self._collect_system_metrics()
                self._check_workflow_health()
                self._update_health_status()
                self._process_alerts()
                time.sleep(self.config.health_check_interval)
            except Exception as e:
                logger.error(f"Monitoring loop error: {e}")
                time.sleep(5)  # Brief pause before retrying
    
    def _collect_system_metrics(self):
        """Collect system-level metrics"""
        try:
            self.system_metrics.update({
                'timestamp': datetime.now().isoformat(),
                'cpu_percent': psutil.cpu_percent(interval=1),
                'memory_percent': psutil.virtual_memory().percent,
                'disk_percent': psutil.disk_usage('/').percent,
                'active_workflows': len(self.active_workflows),
                'completed_workflows': len(self.completed_workflows),
                'system_load': psutil.getloadavg()[0] if hasattr(psutil, 'getloadavg') else 0,
                'available_memory_gb': psutil.virtual_memory().available / (1024**3)
            })
        except Exception as e:
            logger.error(f"System metrics collection failed: {e}")
    
    def _check_workflow_health(self):
        """Check health of active workflows"""
        current_time = datetime.now()
        
        for workflow_id, metrics in list(self.active_workflows.items()):
            # Check for stuck workflows
            execution_time = (current_time - metrics.start_time).total_seconds()
            
            if execution_time > self.config.default_timeout_seconds:
                self._create_alert(
                    "workflow_timeout",
                    f"Workflow {workflow_id} has exceeded timeout threshold",
                    "warning"
                )
            
            # Check resource usage
            if metrics.cpu_usage_peak > 90:
                self._create_alert(
                    "high_cpu_usage",
                    f"Workflow {workflow_id} using {metrics.cpu_usage_peak}% CPU",
                    "warning"
                )
            
            if metrics.memory_usage_peak > 90:
                self._create_alert(
                    "high_memory_usage",
                    f"Workflow {workflow_id} using {metrics.memory_usage_peak}% memory",
                    "warning"
                )
    
    def _update_health_status(self):
        """Update overall system health status"""
        try:
            cpu_usage = self.system_metrics.get('cpu_percent', 0)
            memory_usage = self.system_metrics.get('memory_percent', 0)
            active_workflows = self.system_metrics.get('active_workflows', 0)
            
            if cpu_usage > 90 or memory_usage > 90:
                self.health_status = "critical"
            elif cpu_usage > 80 or memory_usage > 80 or active_workflows > self.config.max_concurrent_workflows * 0.9:
                self.health_status = "warning"
            else:
                self.health_status = "healthy"
                
        except Exception as e:
            logger.error(f"Health status update failed: {e}")
            self.health_status = "unknown"
    
    def _create_alert(self, alert_type: str, message: str, severity: str):
        """Create system alert"""
        alert = {
            'timestamp': datetime.now().isoformat(),
            'type': alert_type,
            'message': message,
            'severity': severity,
            'id': str(uuid.uuid4())
        }
        
        self.alerts.append(alert)
        logger.warning(f"ALERT [{severity.upper()}] {alert_type}: {message}")
        
        # Keep only recent alerts
        if len(self.alerts) > 1000:
            self.alerts = self.alerts[-500:]
    
    def _process_alerts(self):
        """Process and escalate alerts as needed"""
        critical_alerts = [a for a in self.alerts if a.get('severity') == 'critical']
        
        if len(critical_alerts) > 0:
            logger.critical(f"CRITICAL ALERTS DETECTED: {len(critical_alerts)} critical alerts active")
    
    def register_workflow_start(self, workflow_id: str, steps_total: int, priority: WorkflowPriority = WorkflowPriority.NORMAL) -> WorkflowMetrics:
        """Register workflow start and begin monitoring"""
        metrics = WorkflowMetrics(
            workflow_id=workflow_id,
            start_time=datetime.now(),
            status=WorkflowStatus.RUNNING,
            priority=priority,
            steps_total=steps_total
        )
        
        self.active_workflows[workflow_id] = metrics
        logger.info(f"Workflow {workflow_id} registered for monitoring")
        return metrics
    
    def update_workflow_progress(self, workflow_id: str, steps_completed: int, custom_metrics: Dict[str, Any] = None):
        """Update workflow progress metrics"""
        if workflow_id in self.active_workflows:
            metrics = self.active_workflows[workflow_id]
            metrics.steps_completed = steps_completed
            
            if custom_metrics:
                metrics.custom_metrics.update(custom_metrics)
            
            # Update resource usage
            metrics.cpu_usage_peak = max(metrics.cpu_usage_peak, psutil.cpu_percent())
            metrics.memory_usage_peak = max(metrics.memory_usage_peak, psutil.virtual_memory().percent)
    
    def complete_workflow(self, workflow_id: str, success: bool = True, error_count: int = 0, quality_score: float = 1.0):
        """Mark workflow as completed and calculate final metrics"""
        if workflow_id in self.active_workflows:
            metrics = self.active_workflows[workflow_id]
            metrics.end_time = datetime.now()
            metrics.status = WorkflowStatus.COMPLETED if success else WorkflowStatus.FAILED
            metrics.error_count = error_count
            metrics.quality_score = quality_score
            metrics.execution_time_seconds = (metrics.end_time - metrics.start_time).total_seconds()
            metrics.success_rate = (metrics.steps_completed - error_count) / max(metrics.steps_completed, 1)
            
            # Move to completed workflows
            self.completed_workflows.append(metrics)
            del self.active_workflows[workflow_id]
            
            logger.info(f"Workflow {workflow_id} completed - Success: {success}, Duration: {metrics.execution_time_seconds:.2f}s")
    
    def get_system_health_report(self) -> Dict[str, Any]:
        """Generate comprehensive system health report"""
        recent_workflows = [w for w in self.completed_workflows if (datetime.now() - w.end_time).total_seconds() < 3600]
        
        return {
            "health_status": self.health_status,
            "timestamp": datetime.now().isoformat(),
            "system_metrics": self.system_metrics,
            "active_workflows": len(self.active_workflows),
            "completed_workflows_last_hour": len(recent_workflows),
            "average_execution_time": sum(w.execution_time_seconds for w in recent_workflows) / max(len(recent_workflows), 1),
            "success_rate": sum(1 for w in recent_workflows if w.status == WorkflowStatus.COMPLETED) / max(len(recent_workflows), 1),
            "recent_alerts": self.alerts[-10:],
            "performance_summary": {
                "peak_cpu": max((w.cpu_usage_peak for w in recent_workflows), default=0),
                "peak_memory": max((w.memory_usage_peak for w in recent_workflows), default=0),
                "avg_quality_score": sum(w.quality_score for w in recent_workflows) / max(len(recent_workflows), 1)
            }
        }

class ProductionWorkflowExecutor:
    """Enterprise-grade workflow executor with comprehensive production features"""
    
    def __init__(self, config: ProductionConfig):
        self.config = config
        self.monitor = ProductionWorkflowMonitor(config)
        self.execution_queue = []
        self.circuit_breakers = {}
        self.rate_limiters = {}
        self.security_validator = ProductionSecurityValidator()
        
        # Start monitoring
        if config.performance_monitoring_enabled:
            self.monitor.start_monitoring()
    
    def shutdown(self):
        """Gracefully shutdown the production executor"""
        logger.info("Shutting down production workflow executor")
        self.monitor.stop_monitoring()
    
    def execute_production_workflow(
        self,
        workflow: Workflow,
        input_message: str,
        priority: WorkflowPriority = WorkflowPriority.NORMAL,
        timeout_seconds: Optional[int] = None,
        retry_policy: Optional[Dict[str, Any]] = None,
        security_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Execute workflow with full production capabilities"""
        
        workflow_id = str(uuid.uuid4())
        timeout = timeout_seconds or self.config.default_timeout_seconds
        
        try:
            logger.info(f"Starting production workflow execution: {workflow_id}")
            
            # Security validation
            if self.config.security_validation_enabled:
                self._validate_security_context(input_message, security_context)
            
            # Rate limiting
            if self.config.rate_limiting_enabled:
                self._apply_rate_limiting(workflow.name)
            
            # Circuit breaker check
            if self.config.circuit_breaker_enabled:
                self._check_circuit_breaker(workflow.name)
            
            # Register workflow monitoring
            steps_total = len(workflow.steps) if hasattr(workflow, 'steps') else 1
            metrics = self.monitor.register_workflow_start(workflow_id, steps_total, priority)
            
            # Execute workflow with comprehensive monitoring
            execution_result = self._execute_with_monitoring(
                workflow, input_message, workflow_id, timeout, retry_policy
            )
            
            # Complete monitoring
            self.monitor.complete_workflow(
                workflow_id,
                success=execution_result.get('success', True),
                error_count=execution_result.get('error_count', 0),
                quality_score=execution_result.get('quality_score', 1.0)
            )
            
            return {
                "workflow_id": workflow_id,
                "success": True,
                "result": execution_result,
                "metrics": asdict(metrics),
                "execution_summary": self._generate_execution_summary(workflow_id, execution_result)
            }
            
        except Exception as e:
            logger.error(f"Production workflow execution failed: {workflow_id}: {e}")
            self.monitor.complete_workflow(workflow_id, success=False, error_count=1)
            
            return {
                "workflow_id": workflow_id,
                "success": False,
                "error": str(e),
                "error_details": {
                    "traceback": traceback.format_exc(),
                    "error_type": type(e).__name__
                }
            }
    
    def _execute_with_monitoring(
        self,
        workflow: Workflow,
        input_message: str,
        workflow_id: str,
        timeout: int,
        retry_policy: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Execute workflow with comprehensive monitoring and error handling"""
        
        start_time = time.time()
        retry_attempts = 0
        max_retries = retry_policy.get('max_retries', self.config.retry_attempts) if retry_policy else self.config.retry_attempts
        
        while retry_attempts <= max_retries:
            try:
                # Execute workflow with timeout
                with self._timeout_context(timeout):
                    result = workflow.run(input_message, stream=False)
                    
                    execution_time = time.time() - start_time
                    
                    # Update progress
                    self.monitor.update_workflow_progress(
                        workflow_id,
                        steps_completed=1,  # Simplified - in real implementation, track actual steps
                        custom_metrics={
                            "execution_time": execution_time,
                            "retry_attempts": retry_attempts,
                            "final_attempt": True
                        }
                    )
                    
                    return {
                        "success": True,
                        "content": getattr(result, 'content', str(result)),
                        "execution_time": execution_time,
                        "retry_attempts": retry_attempts,
                        "quality_score": 1.0,  # Could be calculated based on result analysis
                        "error_count": 0
                    }
                    
            except Exception as e:
                retry_attempts += 1
                logger.warning(f"Workflow execution attempt {retry_attempts} failed: {e}")
                
                if retry_attempts > max_retries:
                    raise e
                
                # Apply retry backoff
                backoff_time = self.config.retry_backoff_multiplier ** retry_attempts
                logger.info(f"Retrying in {backoff_time} seconds...")
                time.sleep(backoff_time)
                
                # Update monitoring
                self.monitor.update_workflow_progress(
                    workflow_id,
                    steps_completed=0,
                    custom_metrics={"retry_attempt": retry_attempts, "last_error": str(e)}
                )
        
        # Should not reach here, but fallback
        raise Exception(f"Workflow failed after {max_retries} retries")
    
    @contextmanager
    def _timeout_context(self, timeout_seconds: int):
        """Context manager for workflow execution timeout"""
        # Simplified timeout implementation
        # In production, would use more sophisticated timeout handling
        start_time = time.time()
        try:
            yield
            execution_time = time.time() - start_time
            if execution_time > timeout_seconds:
                raise TimeoutError(f"Workflow execution exceeded {timeout_seconds} seconds")
        except Exception as e:
            execution_time = time.time() - start_time
            if execution_time > timeout_seconds:
                raise TimeoutError(f"Workflow execution exceeded {timeout_seconds} seconds")
            raise e
    
    def _validate_security_context(self, input_message: str, security_context: Optional[Dict[str, Any]]):
        """Validate security context and input"""
        if self.security_validator:
            self.security_validator.validate_input(input_message, security_context)
    
    def _apply_rate_limiting(self, workflow_name: str):
        """Apply rate limiting for workflow execution"""
        # Simplified rate limiting implementation
        current_time = time.time()
        rate_limit_key = f"workflow_{workflow_name}"
        
        if rate_limit_key not in self.rate_limiters:
            self.rate_limiters[rate_limit_key] = {"count": 0, "window_start": current_time}
        
        rate_data = self.rate_limiters[rate_limit_key]
        
        # Reset window if needed (1-minute window)
        if current_time - rate_data["window_start"] > 60:
            rate_data["count"] = 0
            rate_data["window_start"] = current_time
        
        # Check rate limit (max 60 per minute per workflow type)
        if rate_data["count"] >= 60:
            raise Exception(f"Rate limit exceeded for workflow {workflow_name}")
        
        rate_data["count"] += 1
    
    def _check_circuit_breaker(self, workflow_name: str):
        """Check circuit breaker status for workflow"""
        circuit_key = f"circuit_{workflow_name}"
        
        if circuit_key in self.circuit_breakers:
            breaker = self.circuit_breakers[circuit_key]
            
            if breaker["status"] == "open":
                # Check if circuit should be half-open
                if time.time() - breaker["last_failure"] > 300:  # 5 minute timeout
                    breaker["status"] = "half-open"
                    logger.info(f"Circuit breaker for {workflow_name} moved to half-open")
                else:
                    raise Exception(f"Circuit breaker open for workflow {workflow_name}")
    
    def _update_circuit_breaker(self, workflow_name: str, success: bool):
        """Update circuit breaker state based on execution result"""
        circuit_key = f"circuit_{workflow_name}"
        
        if circuit_key not in self.circuit_breakers:
            self.circuit_breakers[circuit_key] = {
                "status": "closed",
                "failure_count": 0,
                "last_failure": 0,
                "success_count": 0
            }
        
        breaker = self.circuit_breakers[circuit_key]
        
        if success:
            breaker["success_count"] += 1
            breaker["failure_count"] = max(0, breaker["failure_count"] - 1)
            
            if breaker["status"] == "half-open" and breaker["success_count"] >= 3:
                breaker["status"] = "closed"
                logger.info(f"Circuit breaker for {workflow_name} closed")
        else:
            breaker["failure_count"] += 1
            breaker["last_failure"] = time.time()
            breaker["success_count"] = 0
            
            if breaker["failure_count"] >= 5:  # Open after 5 failures
                breaker["status"] = "open"
                logger.warning(f"Circuit breaker for {workflow_name} opened due to failures")
    
    def _generate_execution_summary(self, workflow_id: str, execution_result: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive execution summary"""
        return {
            "workflow_id": workflow_id,
            "execution_time_seconds": execution_result.get("execution_time", 0),
            "retry_attempts": execution_result.get("retry_attempts", 0),
            "quality_score": execution_result.get("quality_score", 1.0),
            "success_rate": 1.0 if execution_result.get("success") else 0.0,
            "error_count": execution_result.get("error_count", 0),
            "production_features_applied": {
                "monitoring": True,
                "retry_policy": execution_result.get("retry_attempts", 0) > 0,
                "circuit_breaker": self.config.circuit_breaker_enabled,
                "rate_limiting": self.config.rate_limiting_enabled,
                "security_validation": self.config.security_validation_enabled
            }
        }
    
    def get_production_status(self) -> Dict[str, Any]:
        """Get comprehensive production system status"""
        return {
            "system_health": self.monitor.get_system_health_report(),
            "configuration": asdict(self.config),
            "circuit_breakers": {k: v for k, v in self.circuit_breakers.items()},
            "active_workflows": len(self.monitor.active_workflows),
            "total_completed": len(self.monitor.completed_workflows),
            "alerts_active": len([a for a in self.monitor.alerts if (datetime.now() - datetime.fromisoformat(a['timestamp'])).total_seconds() < 3600])
        }

class ProductionSecurityValidator:
    """Production security validation for workflow inputs and contexts"""
    
    def __init__(self):
        self.blocked_patterns = [
            r'<script\b[^<]*(?:(?!<\/script>)<[^<]*)*<\/script>',  # XSS patterns
            r'(?i)\b(?:union|select|insert|delete|drop|create|alter)\b.*\b(?:from|into|table)\b',  # SQL injection
            r'(?i)(?:eval|exec|system|shell_exec|passthru)\s*\(',  # Code injection
        ]
    
    def validate_input(self, input_message: str, security_context: Optional[Dict[str, Any]] = None):
        """Validate input for security threats"""
        
        # Check for malicious patterns
        import re
        for pattern in self.blocked_patterns:
            if re.search(pattern, input_message, re.IGNORECASE):
                raise SecurityError(f"Potentially malicious input detected")
        
        # Validate input length
        if len(input_message) > 10000:  # 10KB limit
            raise SecurityError("Input exceeds maximum allowed length")
        
        # Additional security validations
        if security_context:
            self._validate_security_context(security_context)
    
    def _validate_security_context(self, security_context: Dict[str, Any]):
        """Validate security context parameters"""
        required_fields = ['user_id', 'session_token']
        
        for field in required_fields:
            if field not in security_context:
                raise SecurityError(f"Missing required security field: {field}")

class SecurityError(Exception):
    """Custom security validation error"""
    pass

# Define production-grade agents with enhanced configuration
production_market_researcher = Agent(
    name="Production Market Research Specialist",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    role="Conduct enterprise-grade market research with comprehensive analysis",
    instructions=[
        "Provide institutional-quality market research with quantitative analysis",
        "Include confidence intervals and data quality assessments",
        "Focus on actionable insights for executive decision making",
        "Validate all data sources and provide source attribution"
    ]
)

production_business_strategist = Agent(
    name="Production Business Strategy Consultant",
    model=OpenAIChat(id="gpt-4o"),
    role="Develop enterprise business strategies with comprehensive risk analysis",
    instructions=[
        "Create detailed strategic recommendations with implementation roadmaps",
        "Include financial projections and ROI analysis",
        "Assess strategic risks and provide mitigation strategies",
        "Align recommendations with enterprise governance requirements"
    ]
)

production_technical_architect = Agent(
    name="Production Technical Architecture Specialist",
    model=OpenAIChat(id="gpt-4o"),
    role="Design enterprise technical solutions with scalability and security focus",
    instructions=[
        "Provide enterprise-grade technical architecture recommendations",
        "Include security, compliance, and scalability considerations",
        "Design for high availability and disaster recovery",
        "Consider total cost of ownership and operational complexity"
    ]
)

production_quality_assurance = Agent(
    name="Production Quality Assurance Specialist",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Validate workflow outputs for quality, accuracy, and completeness",
    instructions=[
        "Conduct comprehensive quality assessment of all workflow outputs",
        "Validate accuracy, completeness, and consistency",
        "Identify gaps, inconsistencies, and improvement opportunities",
        "Provide quality scores and improvement recommendations"
    ]
)

# Production workflow step definitions
def production_initialization_function(step_input: StepInput) -> StepOutput:
    """Production-grade workflow initialization with comprehensive setup"""
    
    try:
        logger.info("Starting production workflow initialization")
        
        # Comprehensive initialization
        initialization_context = {
            "request": step_input.message,
            "timestamp": datetime.now().isoformat(),
            "workflow_type": "enterprise_analysis",
            "quality_requirements": "institutional_grade",
            "security_level": "high",
            "compliance_required": True,
            "audit_trail_enabled": True
        }
        
        # Security validation
        if len(step_input.message) < 10:
            raise ValueError("Request too brief for comprehensive analysis")
        
        if len(step_input.message) > 5000:
            raise ValueError("Request exceeds maximum length for processing")
        
        initialization_output = f"""
        # Production Workflow Initialization Complete
        
        **Enterprise Initialization Summary**
        - Request: {step_input.message}
        - Workflow Type: Enterprise Strategic Analysis
        - Quality Requirements: Institutional Grade
        - Security Level: High
        - Compliance: Required
        - Audit Trail: Enabled
        
        ## Production Configuration
        - Processing Mode: Multi-agent enterprise analysis
        - Quality Assurance: Integrated validation at each step
        - Security Validation: Input sanitization and context validation
        - Performance Monitoring: Real-time metrics and alerting
        - Error Handling: Comprehensive retry and fallback strategies
        
        ## Workflow Readiness Status
        - ✓ Security validation passed
        - ✓ Input validation completed
        - ✓ Resource allocation confirmed
        - ✓ Quality framework initialized
        - ✓ Monitoring systems active
        - ✓ Audit logging enabled
        
        ## Production Ready
        Enterprise workflow initialization completed successfully. System ready for comprehensive analysis execution.
        """
        
        return StepOutput(
            content=initialization_output,
            metadata={
                "initialization_context": initialization_context,
                "production_ready": True,
                "security_validated": True,
                "quality_framework": "enabled"
            }
        )
        
    except Exception as e:
        logger.error(f"Production initialization failed: {e}")
        return StepOutput(
            content=f"Production initialization failed: {str(e)}",
            success=False,
            error=str(e)
        )

def production_analysis_orchestrator_function(step_input: StepInput) -> StepOutput:
    """Production-grade analysis orchestration with comprehensive quality assurance"""
    
    try:
        logger.info("Starting production analysis orchestration")
        
        # Multi-agent analysis execution with error handling
        analysis_results = {}
        quality_scores = {}
        
        # Market research analysis
        try:
            market_prompt = f"""
            ENTERPRISE MARKET ANALYSIS:
            
            Request: {step_input.message}
            
            Requirements:
            1. Institutional-quality market research with quantitative analysis
            2. Include market size, growth rates, and competitive dynamics
            3. Provide data quality assessments and confidence intervals
            4. Include source attribution and validation
            5. Format for executive consumption
            
            Execute comprehensive market analysis.
            """
            
            market_result = production_market_researcher.run(market_prompt)
            analysis_results["market_research"] = market_result.content
            quality_scores["market_research"] = 0.95  # High quality expected
            
        except Exception as e:
            logger.error(f"Market research analysis failed: {e}")
            analysis_results["market_research"] = f"Market analysis unavailable due to: {str(e)}"
            quality_scores["market_research"] = 0.0
        
        # Business strategy analysis
        try:
            strategy_prompt = f"""
            ENTERPRISE BUSINESS STRATEGY ANALYSIS:
            
            Request: {step_input.message}
            Market Context: {analysis_results.get("market_research", "Not available")[:500]}...
            
            Requirements:
            1. Comprehensive strategic analysis with implementation roadmap
            2. Financial projections and ROI analysis
            3. Risk assessment and mitigation strategies
            4. Alignment with enterprise governance requirements
            5. Executive-ready strategic recommendations
            
            Execute comprehensive business strategy analysis.
            """
            
            strategy_result = production_business_strategist.run(strategy_prompt)
            analysis_results["business_strategy"] = strategy_result.content
            quality_scores["business_strategy"] = 0.92
            
        except Exception as e:
            logger.error(f"Business strategy analysis failed: {e}")
            analysis_results["business_strategy"] = f"Strategy analysis unavailable due to: {str(e)}"
            quality_scores["business_strategy"] = 0.0
        
        # Technical architecture analysis
        try:
            technical_prompt = f"""
            ENTERPRISE TECHNICAL ARCHITECTURE ANALYSIS:
            
            Request: {step_input.message}
            Business Context: {analysis_results.get("business_strategy", "Not available")[:500]}...
            
            Requirements:
            1. Enterprise-grade technical architecture recommendations
            2. Security, compliance, and scalability considerations
            3. High availability and disaster recovery design
            4. Total cost of ownership analysis
            5. Implementation and operational complexity assessment
            
            Execute comprehensive technical architecture analysis.
            """
            
            technical_result = production_technical_architect.run(technical_prompt)
            analysis_results["technical_architecture"] = technical_result.content
            quality_scores["technical_architecture"] = 0.90
            
        except Exception as e:
            logger.error(f"Technical architecture analysis failed: {e}")
            analysis_results["technical_architecture"] = f"Technical analysis unavailable due to: {str(e)}"
            quality_scores["technical_architecture"] = 0.0
        
        # Quality assurance validation
        try:
            qa_prompt = f"""
            COMPREHENSIVE QUALITY ASSURANCE VALIDATION:
            
            Original Request: {step_input.message}
            
            Analysis Results to Validate:
            
            Market Research: {analysis_results.get("market_research", "Not available")[:400]}...
            
            Business Strategy: {analysis_results.get("business_strategy", "Not available")[:400]}...
            
            Technical Architecture: {analysis_results.get("technical_architecture", "Not available")[:400]}...
            
            Quality Validation Requirements:
            1. Assess accuracy, completeness, and consistency across all analyses
            2. Identify gaps, inconsistencies, and improvement opportunities
            3. Validate alignment with original request requirements
            4. Provide overall quality score and specific improvement recommendations
            5. Ensure institutional-grade quality standards
            
            Execute comprehensive quality validation.
            """
            
            qa_result = production_quality_assurance.run(qa_prompt)
            quality_validation = qa_result.content
            overall_quality_score = sum(quality_scores.values()) / len(quality_scores) if quality_scores else 0.0
            
        except Exception as e:
            logger.error(f"Quality assurance validation failed: {e}")
            quality_validation = f"Quality validation unavailable due to: {str(e)}"
            overall_quality_score = 0.5  # Default moderate score
        
        # Generate comprehensive production output
        production_output = f"""
        # Enterprise Production Analysis Results
        
        **Executive Summary**
        - Analysis Request: {step_input.message}
        - Analysis Scope: Comprehensive multi-dimensional enterprise analysis
        - Quality Level: Institutional grade
        - Analysis Components: {len([k for k, v in analysis_results.items() if not v.startswith("analysis unavailable")])} of {len(analysis_results)} completed successfully
        - Overall Quality Score: {overall_quality_score:.2%}
        
        ## Market Research Analysis
        **Quality Score**: {quality_scores.get('market_research', 0):.1%}
        
        {analysis_results.get('market_research', 'Market research analysis not available')}
        
        ## Business Strategy Analysis
        **Quality Score**: {quality_scores.get('business_strategy', 0):.1%}
        
        {analysis_results.get('business_strategy', 'Business strategy analysis not available')}
        
        ## Technical Architecture Analysis
        **Quality Score**: {quality_scores.get('technical_architecture', 0):.1%}
        
        {analysis_results.get('technical_architecture', 'Technical architecture analysis not available')}
        
        ## Quality Assurance Validation
        **Overall Quality Assessment**
        
        {quality_validation}
        
        ## Production Workflow Performance
        - Multi-agent Coordination: ✓ {len(analysis_results)} analysis dimensions
        - Quality Assurance: ✓ Integrated validation framework
        - Error Handling: ✓ Graceful degradation for failed components
        - Enterprise Standards: ✓ Institutional-grade analysis completed
        - Audit Trail: ✓ Complete execution tracking
        
        ## Executive Recommendation
        This comprehensive analysis provides institutional-quality insights across market, strategic, and technical dimensions. 
        The analysis meets enterprise standards with an overall quality score of {overall_quality_score:.1%}.
        
        Recommended next steps based on analysis findings and strategic priorities identified.
        """
        
        return StepOutput(
            content=production_output,
            metadata={
                "production_analysis": True,
                "analysis_components": len(analysis_results),
                "successful_components": len([k for k, v in analysis_results.items() if not v.startswith("analysis unavailable")]),
                "overall_quality_score": overall_quality_score,
                "quality_scores": quality_scores,
                "enterprise_grade": True,
                "audit_trail": True
            }
        )
        
    except Exception as e:
        logger.error(f"Production analysis orchestration failed: {e}")
        return StepOutput(
            content=f"Production analysis orchestration failed: {str(e)}",
            success=False,
            error=str(e)
        )

# Create production workflow configuration
production_config = ProductionConfig(
    max_concurrent_workflows=20,
    max_steps_per_workflow=100,
    default_timeout_seconds=1800,  # 30 minutes
    retry_attempts=3,
    health_check_interval=30,
    metrics_collection_enabled=True,
    performance_monitoring_enabled=True,
    auto_scaling_enabled=True,
    circuit_breaker_enabled=True,
    rate_limiting_enabled=True,
    security_validation_enabled=True,
    audit_logging_enabled=True
)

# Create production executor
production_executor = ProductionWorkflowExecutor(production_config)

# Define production workflow steps
production_initialization_step = Step(
    name="production_initialization",
    executor=production_initialization_function,
    description="Production-grade workflow initialization with comprehensive setup and validation"
)

production_analysis_step = Step(
    name="production_analysis_orchestrator",
    executor=production_analysis_orchestrator_function,
    description="Enterprise analysis orchestration with multi-agent coordination and quality assurance"
)

# Create enterprise production workflow
enterprise_production_workflow = Workflow(
    name="Enterprise Production Analysis Pipeline",
    description="Production-grade enterprise workflow with comprehensive monitoring, quality assurance, and error handling",
    storage=SqliteStorage(
        table_name="production_workflows",
        db_file="enterprise_production.db",
        mode="workflow_v2"
    ),
    steps=[production_initialization_step, production_analysis_step]
)

# Production execution and monitoring
if __name__ == "__main__":
    try:
        logger.info("Starting enterprise production workflow system")
        
        # Test enterprise workflow with comprehensive monitoring
        test_requests = [
            {
                "message": "Analyze the strategic market opportunity for AI-powered healthcare diagnostics in the North American market, including competitive positioning, technical feasibility, regulatory requirements, and 5-year market entry strategy with financial projections",
                "priority": WorkflowPriority.HIGH,
                "security_context": {"user_id": "enterprise_user_001", "session_token": "secure_token_123"}
            },
            {
                "message": "Develop comprehensive digital transformation strategy for traditional manufacturing company transitioning to Industry 4.0 with emphasis on IoT integration, cybersecurity, and workforce development",
                "priority": WorkflowPriority.NORMAL,
                "security_context": {"user_id": "enterprise_user_002", "session_token": "secure_token_456"}
            }
        ]
        
        execution_results = []
        
        for i, request in enumerate(test_requests, 1):
            print(f"\n{'='*80}")
            print(f"Enterprise Production Test {i}")
            print(f"{'='*80}")
            print(f"Request: {request['message'][:100]}...")
            
            # Execute with production executor
            result = production_executor.execute_production_workflow(
                workflow=enterprise_production_workflow,
                input_message=request["message"],
                priority=request["priority"],
                timeout_seconds=900,  # 15 minutes
                security_context=request["security_context"]
            )
            
            execution_results.append(result)
            
            print(f"Workflow ID: {result['workflow_id']}")
            print(f"Success: {result['success']}")
            
            if result["success"]:
                print(f"Execution Time: {result['result']['execution_time']:.2f}s")
                print(f"Quality Score: {result['result']['quality_score']:.2%}")
                print(f"Retry Attempts: {result['result']['retry_attempts']}")
            else:
                print(f"Error: {result['error']}")
        
        # Generate production system status report
        print(f"\n{'='*80}")
        print("Production System Status Report")
        print(f"{'='*80}")
        
        system_status = production_executor.get_production_status()
        
        print(f"System Health: {system_status['system_health']['health_status']}")
        print(f"Active Workflows: {system_status['active_workflows']}")
        print(f"Total Completed: {system_status['total_completed']}")
        print(f"Active Alerts: {system_status['alerts_active']}")
        print(f"Success Rate: {system_status['system_health']['success_rate']:.2%}")
        print(f"Avg Execution Time: {system_status['system_health']['average_execution_time']:.2f}s")
        
        # Display performance metrics
        performance = system_status['system_health']['performance_summary']
        print(f"\nPerformance Summary:")
        print(f"- Peak CPU Usage: {performance['peak_cpu']:.1f}%")
        print(f"- Peak Memory Usage: {performance['peak_memory']:.1f}%")
        print(f"- Average Quality Score: {performance['avg_quality_score']:.2%}")
        
        # Display recent alerts
        if system_status['system_health']['recent_alerts']:
            print(f"\nRecent Alerts:")
            for alert in system_status['system_health']['recent_alerts']:
                print(f"- [{alert['severity'].upper()}] {alert['type']}: {alert['message']}")
        else:
            print("\nNo recent alerts - system operating normally")
        
    except Exception as e:
        logger.error(f"Enterprise production workflow system failed: {e}")
        print(f"System failure: {e}")
        
    finally:
        # Graceful shutdown
        production_executor.shutdown()
        logger.info("Enterprise production workflow system shutdown complete")
```

## Advanced Production Patterns

### Auto-Scaling Workflow Management
```python
class AutoScalingWorkflowManager:
    """Automatically scale workflow execution based on demand and system resources"""
    
    def __init__(self, min_workers: int = 2, max_workers: int = 20):
        self.min_workers = min_workers
        self.max_workers = max_workers
        self.current_workers = min_workers
        self.worker_pool = None
        self.scaling_metrics = {}
        
    def initialize_worker_pool(self):
        """Initialize worker pool with minimum workers"""
        self.worker_pool = ThreadPoolExecutor(max_workers=self.current_workers)
        logger.info(f"Initialized worker pool with {self.current_workers} workers")
    
    def should_scale_up(self) -> bool:
        """Determine if scaling up is needed"""
        # Check queue length and resource utilization
        cpu_usage = psutil.cpu_percent()
        memory_usage = psutil.virtual_memory().percent
        
        # Scale up if high demand and resources available
        return (
            cpu_usage < 70 and 
            memory_usage < 80 and
            self.current_workers < self.max_workers and
            self._queue_length() > self.current_workers * 2
        )
    
    def should_scale_down(self) -> bool:
        """Determine if scaling down is needed"""
        # Scale down if low demand
        return (
            self.current_workers > self.min_workers and
            self._queue_length() < self.current_workers / 2 and
            self._worker_utilization() < 0.3
        )
    
    def scale_up(self):
        """Scale up worker pool"""
        if self.current_workers < self.max_workers:
            new_worker_count = min(self.current_workers + 2, self.max_workers)
            self._resize_worker_pool(new_worker_count)
            logger.info(f"Scaled up to {new_worker_count} workers")
    
    def scale_down(self):
        """Scale down worker pool"""
        if self.current_workers > self.min_workers:
            new_worker_count = max(self.current_workers - 1, self.min_workers)
            self._resize_worker_pool(new_worker_count)
            logger.info(f"Scaled down to {new_worker_count} workers")
    
    def _resize_worker_pool(self, new_size: int):
        """Resize worker pool to new size"""
        if self.worker_pool:
            self.worker_pool.shutdown(wait=False)
        
        self.current_workers = new_size
        self.worker_pool = ThreadPoolExecutor(max_workers=new_size)
    
    def _queue_length(self) -> int:
        """Get current queue length (simplified)"""
        return getattr(self.worker_pool, '_work_queue', queue.Queue()).qsize() if self.worker_pool else 0
    
    def _worker_utilization(self) -> float:
        """Calculate worker utilization (simplified)"""
        if not self.worker_pool:
            return 0.0
        
        active_threads = threading.active_count()
        return min(1.0, active_threads / self.current_workers)
```

### Disaster Recovery and Backup
```python
class ProductionBackupManager:
    """Manage backups and disaster recovery for production workflows"""
    
    def __init__(self, backup_location: str = "backups/"):
        self.backup_location = backup_location
        self.backup_schedule = {}
        
    def create_workflow_backup(self, workflow_id: str, workflow_state: Dict[str, Any]):
        """Create backup of workflow state"""
        try:
            backup_filename = f"{self.backup_location}workflow_{workflow_id}_{int(time.time())}.json"
            
            os.makedirs(os.path.dirname(backup_filename), exist_ok=True)
            
            with open(backup_filename, 'w') as f:
                json.dump(workflow_state, f, indent=2, default=str)
            
            logger.info(f"Workflow backup created: {backup_filename}")
            return backup_filename
            
        except Exception as e:
            logger.error(f"Backup creation failed for {workflow_id}: {e}")
            return None
    
    def restore_workflow_state(self, backup_filename: str) -> Optional[Dict[str, Any]]:
        """Restore workflow state from backup"""
        try:
            with open(backup_filename, 'r') as f:
                workflow_state = json.load(f)
            
            logger.info(f"Workflow state restored from: {backup_filename}")
            return workflow_state
            
        except Exception as e:
            logger.error(f"Backup restoration failed: {e}")
            return None
    
    def cleanup_old_backups(self, retention_days: int = 7):
        """Clean up old backups based on retention policy"""
        try:
            cutoff_time = time.time() - (retention_days * 24 * 3600)
            
            for filename in os.listdir(self.backup_location):
                if filename.endswith('.json'):
                    file_path = os.path.join(self.backup_location, filename)
                    if os.path.getmtime(file_path) < cutoff_time:
                        os.remove(file_path)
                        logger.info(f"Removed old backup: {filename}")
                        
        except Exception as e:
            logger.error(f"Backup cleanup failed: {e}")
```

### Advanced Monitoring and Alerting
```python
class AdvancedMonitoringSystem:
    """Advanced monitoring with custom metrics, alerting, and dashboard generation"""
    
    def __init__(self):
        self.custom_metrics = {}
        self.alert_rules = []
        self.dashboard_data = {}
        
    def register_custom_metric(self, name: str, description: str, metric_type: str = "gauge"):
        """Register custom metric for tracking"""
        self.custom_metrics[name] = {
            "description": description,
            "type": metric_type,
            "values": [],
            "alerts": []
        }
    
    def record_metric(self, name: str, value: float, timestamp: Optional[datetime] = None):
        """Record custom metric value"""
        if name in self.custom_metrics:
            self.custom_metrics[name]["values"].append({
                "value": value,
                "timestamp": timestamp or datetime.now()
            })
            
            # Check alert rules
            self._check_metric_alerts(name, value)
    
    def add_alert_rule(self, metric_name: str, condition: str, threshold: float, severity: str = "warning"):
        """Add alert rule for metric"""
        alert_rule = {
            "metric": metric_name,
            "condition": condition,  # "greater_than", "less_than", "equals"
            "threshold": threshold,
            "severity": severity
        }
        
        self.alert_rules.append(alert_rule)
    
    def _check_metric_alerts(self, metric_name: str, value: float):
        """Check if metric value triggers any alerts"""
        for rule in self.alert_rules:
            if rule["metric"] == metric_name:
                triggered = False
                
                if rule["condition"] == "greater_than" and value > rule["threshold"]:
                    triggered = True
                elif rule["condition"] == "less_than" and value < rule["threshold"]:
                    triggered = True
                elif rule["condition"] == "equals" and value == rule["threshold"]:
                    triggered = True
                
                if triggered:
                    self._trigger_alert(rule, value)
    
    def _trigger_alert(self, rule: Dict, value: float):
        """Trigger alert based on rule"""
        alert_message = f"Metric '{rule['metric']}' {rule['condition']} {rule['threshold']} (current: {value})"
        logger.warning(f"ALERT [{rule['severity'].upper()}]: {alert_message}")
        
        # Could integrate with external alerting systems here
    
    def generate_dashboard_data(self) -> Dict[str, Any]:
        """Generate data for monitoring dashboard"""
        dashboard = {
            "timestamp": datetime.now().isoformat(),
            "metrics_summary": {},
            "alerts_active": len([r for r in self.alert_rules if self._is_rule_active(r)]),
            "system_health": "healthy"  # Simplified
        }
        
        # Summarize each metric
        for name, metric in self.custom_metrics.items():
            recent_values = [v["value"] for v in metric["values"][-10:]]  # Last 10 values
            
            if recent_values:
                dashboard["metrics_summary"][name] = {
                    "current": recent_values[-1],
                    "average": sum(recent_values) / len(recent_values),
                    "min": min(recent_values),
                    "max": max(recent_values),
                    "trend": "stable"  # Could calculate actual trend
                }
        
        self.dashboard_data = dashboard
        return dashboard
    
    def _is_rule_active(self, rule: Dict) -> bool:
        """Check if alert rule is currently active"""
        # Simplified - would check recent metric values
        return False
```

## Speed Tips

### Production Optimization Strategies
- **Connection Pooling**: Reuse database and API connections across workflow executions
- **Caching Layers**: Implement multi-level caching for frequently accessed data
- **Batch Processing**: Group similar operations for improved throughput
- **Resource Pre-allocation**: Pre-allocate resources during low-demand periods
- **Async Operations**: Use async patterns for I/O-bound operations
- **Load Balancing**: Distribute workload across multiple execution nodes

### Production Patterns
```python
# Pattern 1: Connection pooling
class ConnectionPool:
    def __init__(self, max_connections: int = 10):
        self.pool = queue.Queue(maxsize=max_connections)
        for _ in range(max_connections):
            self.pool.put(self.create_connection())
    
    @contextmanager
    def get_connection(self):
        conn = self.pool.get()
        try:
            yield conn
        finally:
            self.pool.put(conn)

# Pattern 2: Result caching
from functools import lru_cache
import redis

class ProductionCache:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
    
    def get_cached_result(self, key: str) -> Optional[str]:
        return self.redis_client.get(key)
    
    def cache_result(self, key: str, result: str, ttl: int = 3600):
        self.redis_client.setex(key, ttl, result)

# Pattern 3: Health check endpoints
def health_check() -> Dict[str, Any]:
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "uptime": time.time() - start_time,
        "active_workflows": len(monitor.active_workflows),
        "system_resources": {
            "cpu": psutil.cpu_percent(),
            "memory": psutil.virtual_memory().percent,
            "disk": psutil.disk_usage('/').percent
        }
    }

# Pattern 4: Graceful shutdown
import signal
import atexit

def graceful_shutdown(signum, frame):
    logger.info("Received shutdown signal, gracefully shutting down...")
    production_executor.shutdown()
    sys.exit(0)

signal.signal(signal.SIGTERM, graceful_shutdown)
signal.signal(signal.SIGINT, graceful_shutdown)
atexit.register(lambda: production_executor.shutdown())
```

## Common Pitfalls (CRITICAL)

### Production Anti-patterns
```python
# ❌ WRONG - No monitoring or error handling
def bad_production_workflow():
    # No monitoring, logging, or error handling!
    result = workflow.run(message)  # Could fail silently!
    return result  # No validation or quality checks!

# ✅ CORRECT - Comprehensive production handling
def good_production_workflow(workflow_id: str, message: str):
    try:
        # Comprehensive monitoring and error handling
        metrics = monitor.register_workflow_start(workflow_id, 1)
        
        with timeout_context(300):  # 5 minute timeout
            result = workflow.run(message)
            
            # Validate result quality
            quality_score = validate_result_quality(result)
            
            monitor.complete_workflow(workflow_id, success=True, quality_score=quality_score)
            return result
            
    except Exception as e:
        monitor.complete_workflow(workflow_id, success=False)
        logger.error(f"Workflow {workflow_id} failed: {e}")
        raise ProductionWorkflowError(f"Workflow execution failed: {e}")

# ❌ WRONG - No resource management
def resource_unaware_production():
    # Unlimited resource consumption!
    for i in range(10000):  # Could overwhelm system!
        heavy_computation()  # No resource checks!

# ✅ CORRECT - Resource-aware production
def resource_aware_production():
    max_concurrent = min(10, psutil.cpu_count())
    
    with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
        # Monitor resource usage
        if psutil.cpu_percent() > 80 or psutil.virtual_memory().percent > 85:
            raise ResourceExhaustionError("System resources exhausted")
        
        # Process with resource limits
        futures = [executor.submit(safe_computation, task) for task in tasks[:max_concurrent]]
        
        for future in as_completed(futures, timeout=300):
            try:
                result = future.result()
                yield result
            except Exception as e:
                logger.error(f"Task failed: {e}")
                # Continue with other tasks
```

### Security and Compliance Issues
```python
# ❌ WRONG - No security validation
def insecure_workflow(user_input: str):
    # Direct execution without validation!
    eval(user_input)  # EXTREMELY DANGEROUS!
    
    # No input sanitization
    result = workflow.run(user_input)  # Could contain malicious content!
    return result

# ✅ CORRECT - Comprehensive security
def secure_workflow(user_input: str, security_context: Dict):
    # Input validation and sanitization
    validator = ProductionSecurityValidator()
    validator.validate_input(user_input, security_context)
    
    # Sanitize input
    sanitized_input = sanitize_user_input(user_input)
    
    # Execute with security context
    with security_context_manager(security_context):
        result = workflow.run(sanitized_input)
        
        # Audit logging
        audit_log.record({
            "action": "workflow_execution",
            "user": security_context.get("user_id"),
            "input_hash": hash(user_input),
            "result_hash": hash(str(result)),
            "timestamp": datetime.now().isoformat()
        })
        
        return result

# ❌ WRONG - No backup or recovery
def no_backup_workflow():
    # No state preservation!
    workflow.run(message)  # State lost if failure occurs!

# ✅ CORRECT - Backup and recovery
def backup_aware_workflow(workflow_id: str, message: str):
    backup_manager = ProductionBackupManager()
    
    try:
        # Create checkpoint before execution
        workflow_state = capture_workflow_state(workflow_id, message)
        backup_file = backup_manager.create_workflow_backup(workflow_id, workflow_state)
        
        # Execute workflow
        result = workflow.run(message)
        
        # Clean up successful execution backup
        if backup_file:
            os.remove(backup_file)
        
        return result
        
    except Exception as e:
        # Attempt recovery from backup
        logger.error(f"Workflow {workflow_id} failed, attempting recovery: {e}")
        
        if backup_file:
            recovered_state = backup_manager.restore_workflow_state(backup_file)
            if recovered_state:
                return attempt_workflow_recovery(recovered_state)
        
        raise e
```

## Best Practices Summary

- **Comprehensive Monitoring**: Implement detailed monitoring with metrics, alerting, and health checks
- **Error Handling**: Design robust error handling with retries, circuit breakers, and graceful degradation
- **Resource Management**: Monitor and manage system resources with auto-scaling and load balancing
- **Security First**: Implement comprehensive security validation, input sanitization, and audit logging
- **Quality Assurance**: Integrate quality validation and scoring throughout workflow execution
- **Backup and Recovery**: Implement comprehensive backup strategies and disaster recovery procedures
- **Performance Optimization**: Use connection pooling, caching, and async patterns for optimal performance
- **Compliance**: Ensure audit trails, compliance reporting, and regulatory requirement adherence