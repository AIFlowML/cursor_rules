---
alwaysApply: true
---

> You are an expert in pytest, modern mocking, and UV-based workflows. You write deterministic tests that cover success (good-day) and failure (bad-day) paths, from unit to integration level.

# Python Testing Best Practices – UV Edition

## 1 Overview

This rule explains how to design, organise, and run autonomous test suites in Python ≥3.11. It assumes the project follows the production rule that uses UV for dependency management.

Goals:
• Establish repeatable test layout
• Cover good-day / bad-day scenarios at every layer
• Enable integration testing (HTTP APIs, DB, external services)
• Ensure tests run with a single `uv run pytest` command

---

## 2 Directory Layout

```text
project/
├── pyproject.toml        # `[project.optional-dependencies.test]` managed by UV
├── uv.lock
└── tests/
    ├── conftest.py       # Shared fixtures / plugins
    ├── unit/
    │   └── test_*.py
    ├── integration/
    │   └── test_*.py
    ├── e2e/
    │   └── test_*.py
    └── data/             # Sample payloads / golden files
```

Optional: keep fixtures or data files under `tests/fixtures/` for large blobs.

---

## 3 UV Setup

Declare test tools in `pyproject.toml`:

```toml
[project.optional-dependencies.test]
pytest = ">=8.0.0"
pytest-cov = ">=5.0.0"
pytest-asyncio = ">=0.23"
httpx = ">=0.27"          # For async API tests
pytest-mock = ">=3.12"
# Add playwright, locust, etc. for e2e if needed
```

Sync once:

```bash
uv sync --group test
```

Run tests anytime without env activation:

```bash
uv run pytest -q
```

---

## 4 Test Taxonomy

| Layer            | Scope                                        | Speed    | Example                      |
| ---------------- | -------------------------------------------- | -------- | ---------------------------- |
| Unit             | Single function/class                        | <10 ms   | `tests/unit/`                |
| Integration      | Component + side resources (DB, cache, HTTP) | ~100 ms  | `tests/integration/`         |
| E2E / Functional | Full system via public interface             | seconds  | `tests/e2e/`                 |
| Property / Fuzz  | Data invariants                              | variable | `hypothesis` tests           |
| Performance      | Throughput / latency                         | minutes  | `locust`, `pytest-benchmark` |

Always start with fast unit coverage, then add deeper layers selectively.

---

## 5 Good-Day / Bad-Day Pattern

For every behaviour you test:

1. Good-Day = expected inputs → success path.
2. Bad-Day = malformed input, edge case, or failing dependency → graceful error.

### 5.1 Example (Pure Function)

```python
# src/app/utils/math.py
def divide(a: float, b: float) -> float:
    if b == 0:
        raise ZeroDivisionError("Cannot divide by zero")
    return a / b

# tests/unit/test_math.py
import pytest
from app.utils.math import divide

def test_divide_good_day():
    assert divide(10, 2) == 5

@pytest.mark.parametrize("b", [0, -0.0])
def test_divide_bad_day(b):
    with pytest.raises(ZeroDivisionError):
        divide(1, b)
```

### 5.2 Example (Async HTTP Endpoint)

```python
# tests/integration/test_users_api.py
import pytest, httpx, asyncio
from fastapi import FastAPI
from app.main import app   # your FastAPI instance

@pytest.fixture(scope="session")
def anyio_backend():
    return "asyncio"  # Required by pytest-asyncio >= 0.23

@pytest.fixture(scope="module")
async def api_client():
    async with httpx.AsyncClient(app=app, base_url="http://test") as client:
        yield client

@pytest.mark.asyncio
aio
async def test_create_user_good_day(api_client):
    r = await api_client.post("/users", json={"email": "a@b.com"})
    assert r.status_code == 201
    payload = r.json()
    assert payload["email"] == "a@b.com"

@pytest.mark.asyncio
aio
async def test_create_user_bad_day(api_client):
    r = await api_client.post("/users", json={"email": "bad"})
    assert r.status_code == 422  # FastAPI validation error
```

---

## 6 Shared Fixtures & Plugins

Create powerful, reusable fixtures in `conftest.py`.

### 6.1 Database Fixture (PostgreSQL)

```python
# tests/conftest.py
import asyncio, os, pytest, asyncpg

@pytest.fixture(scope="session")
def event_loop():  # Override default loop per session
    loop = asyncio.new_event_loop()
    yield loop
    loop.close()

@pytest.fixture(scope="session", name="pg_dsn")
async def _pg_container(worker_id: str | None = None):
    # Use testcontainers or docker-compose; simplified here
    dsn = os.getenv("TEST_DATABASE_DSN", "postgresql://postgres:pass@localhost:5433/test")
    yield dsn
    # Teardown logic if containerised

@pytest.fixture()
async def db(pg_dsn):
    conn = await asyncpg.connect(pg_dsn)
    try:
        await conn.execute("TRUNCATE TABLE users RESTART IDENTITY CASCADE;")
        yield conn
    finally:
        await conn.close()
```

Use `db` fixture in tests for isolation.

---

## 7 Mocking Guidelines

- Prefer **pytest-mock** (`mocker` fixture) or **unittest.mock**.
- Mock I/O boundaries only—never pure logic.
- Use `MonkeyPatch` for env var injection.

```python
# tests/unit/test_service.py
def test_service_bad_day(mocker):
    mocker.patch("app.services.mailer.send", side_effect=TimeoutError)
    ...
```

---

## 8 Parametrisation & Data-Driven Tests

```python
@pytest.mark.parametrize("raw,expected", [
    ("  spam ", "spam"),
    ("eggs\n", "eggs"),
])
def test_strip(raw, expected):
    from app.utils.text import strip
    assert strip(raw) == expected
```

Use [`pytest-cases`](https://smarie.github.io/python-pytest-cases/) or fixtures returning callables for advanced matrixes.

---

## 9 Assertion Style

- Use bare `assert` for clarity.
- Match full objects with `==` when they implement `__eq__`.
- For floating point, use `pytest.approx`.

---

## 10 Coverage & Quality Gates

```bash
uv run pytest --cov=src --cov-report=xml --cov-fail-under=90
```

Fail CI if below target coverage or if any test is `xfail` unexpectedly.

---

## 11 Performance & Stress Testing (Optional)

Add `pytest-benchmark` or `locust` under `performance` extras.

```bash
uv sync --extra performance
uv run pytest --benchmark-only
```

---

## 12 Continuous Integration

### 12.1 GitHub Actions

```yaml
name: tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/setup-uv@v3
      - run: uv sync --group test
      - run: uv run pytest --cov=src --cov-report=term-missing
```

### 12.2 Pre-commit Hook

```yaml
- repo: local
  hooks:
    - id: pytest
      entry: uv run pytest -q
      language: system
      pass_filenames: false
```

---

## 13 Running Tests Locally

```bash
# Quick feedback
uv run pytest -q  tests/unit

# Full suite with coverage & summary
uv run pytest -ra --cov=src
```

Use `-m "not slow"` marker to skip lengthy tests.

---

## 14 Reliability Tips

1. Each test must be **idempotent**; clean up side effects.
2. Seed RNGs (`random.seed(0)`, `numpy.random.seed(0)`).
3. Avoid datetime flakiness with freezegun.
4. If using Dockerised services, wait for health before running.

---

## 15 References

- [pytest Official Docs](https://docs.pytest.org/)
- [pytest-best-practices](https://docs.pytest.org/en/latest/how-to/best-practices.html)
- [Testcontainers-Python](https://github.com/testcontainers/testcontainers-python)
- [Hypothesis – Property Testing](https://hypothesis.readthedocs.io/)
- [AsyncIO Testing](https://pytest-asyncio.readthedocs.io/)
